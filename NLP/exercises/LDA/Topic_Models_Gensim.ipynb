{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af6e646",
   "metadata": {},
   "source": [
    "## Building Topic Models with the Gensim Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6574edc7",
   "metadata": {},
   "source": [
    "For this notebook, we'll see how to fit different types of topic models using the gensim library. We'll be visualizing the results of our Latent Dirichlet Algorithm, so we'll need to install the pyLDAvis library, which we can do from conda-forge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c15fa856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%conda install -c conda-forge pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8affe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import gensim\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f4ab64",
   "metadata": {},
   "source": [
    "For this notebook, we'll be using abstracts from all machine learning papers posted on arxiv.org since the beginning of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8eb3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = pd.read_csv('ml_papers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e6fce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>categories</th>\n",
       "      <th>abstract</th>\n",
       "      <th>doi</th>\n",
       "      <th>created</th>\n",
       "      <th>updated</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1107.3689</td>\n",
       "      <td>edit wars in wikipedia</td>\n",
       "      <td>stat.ml cs.dl physics.data-an physics.soc-ph</td>\n",
       "      <td>we present a new, efficient method for automat...</td>\n",
       "      <td>10.1109/passat/socialcom.2011.47</td>\n",
       "      <td>2011-07-19</td>\n",
       "      <td>2012-02-09</td>\n",
       "      <td>['r칩bert sumi', 'taha yasseri', 'andr치s rung',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1212.1108</td>\n",
       "      <td>on the convergence properties of optimal adaboost</td>\n",
       "      <td>cs.lg cs.ai stat.ml</td>\n",
       "      <td>adaboost is one of the most popular ml algorit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-12-05</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>['joshua belanich', 'luis e. ortiz']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0  1107.3689                             edit wars in wikipedia   \n",
       "1  1212.1108  on the convergence properties of optimal adaboost   \n",
       "\n",
       "                                     categories  \\\n",
       "0  stat.ml cs.dl physics.data-an physics.soc-ph   \n",
       "1                           cs.lg cs.ai stat.ml   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  we present a new, efficient method for automat...   \n",
       "1  adaboost is one of the most popular ml algorit...   \n",
       "\n",
       "                                doi     created     updated  \\\n",
       "0  10.1109/passat/socialcom.2011.47  2011-07-19  2012-02-09   \n",
       "1                               NaN  2012-12-05  2023-01-04   \n",
       "\n",
       "                                             authors  \n",
       "0  ['r칩bert sumi', 'taha yasseri', 'andr치s rung',...  \n",
       "1               ['joshua belanich', 'luis e. ortiz']  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839adb14",
   "metadata": {},
   "source": [
    "You can change the index number to preview some of the paper abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42492bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: collaborative nested sampling: big data vs. complex physical models\n",
      "----------\n",
      "Abstract: the data torrent unleashed by current and upcoming astronomical surveys demands scalable analysis methods. many machine learning approaches scale well, but separating the instrument measurement from the physical effects of interest, dealing with variable errors, and deriving parameter uncertainties is often an after-thought. classic forward-folding analyses with markov chain monte carlo or nested sampling enable parameter estimation and model comparison, even for complex and slow-to-evaluate physical models. however, these approaches require independent runs for each data set, implying an unfeasible number of model evaluations in the big data regime. here i present a new algorithm, collaborative nested sampling, for deriving parameter probability distributions for each observation. importantly, the number of physical model evaluations scales sub-linearly with the number of data sets, and no assumptions about homogeneous errors, gaussianity, the form of the model or heterogeneity/completeness of the observations need to be made. collaborative nested sampling has immediate application in speeding up analyses of large surveys, integral-field-unit observations, and monte carlo simulations.\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "\n",
    "print(f'Title: {papers.loc[i, \"title\"]}')\n",
    "print('----------')\n",
    "print(f'Abstract: {papers.loc[i, \"abstract\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93aafcf",
   "metadata": {},
   "source": [
    "Before applying any of these documents, we'll need to prepare the documents by preprocessing and tokenizing. For this notebook, we'll use the [simple_preprocess](https://tedboy.github.io/nlps/generated/generated/gensim.utils.simple_preprocess.html) function from the gensim library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69c9eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f16029",
   "metadata": {},
   "source": [
    "Use the simple_simple function to convert the paper abstracts into a list of list of tokens named `docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7a4d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = # fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b1ccad",
   "metadata": {},
   "source": [
    "It's possible that the single tokens that the simple_preprocess function produces will be missing out on some possibly important phrases such as \"machine learning\" or \"convolutional neural network\". We can utilize another tool from gensim to try and automatically uncover such phrases from the text, the [Phrases](https://radimrehurek.com/gensim/models/phrases.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf71cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd88d10",
   "metadata": {},
   "source": [
    "To fit this model, we need to pass in our tokenized documents as the `sentences` argument. We can also specify other hyperparameters. Here, we'll set the minimum count to be 25, meaning these phrases must appear at least 25 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ae2bd37",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3458476304.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    min_count = 25\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "bigram_finder = Phrases(\n",
    "    sentences = # Fill This in\n",
    "    min_count = 25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b976cdc6",
   "metadata": {},
   "source": [
    "Once the model has been fit, we can apply it to a document by passing in the document (as a list of tokens) inside a set of square brackets. Notice that the individual tokens are still present, but two-word phrases are now also listed with the two words separated by an underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "094ed3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'data',\n",
       " 'torrent',\n",
       " 'unleashed',\n",
       " 'by',\n",
       " 'current',\n",
       " 'and',\n",
       " 'upcoming',\n",
       " 'astronomical',\n",
       " 'surveys',\n",
       " 'demands',\n",
       " 'scalable',\n",
       " 'analysis',\n",
       " 'methods',\n",
       " 'many',\n",
       " 'machine_learning',\n",
       " 'approaches',\n",
       " 'scale',\n",
       " 'well',\n",
       " 'but',\n",
       " 'separating',\n",
       " 'the',\n",
       " 'instrument',\n",
       " 'measurement',\n",
       " 'from',\n",
       " 'the',\n",
       " 'physical',\n",
       " 'effects',\n",
       " 'of',\n",
       " 'interest',\n",
       " 'dealing',\n",
       " 'with',\n",
       " 'variable',\n",
       " 'errors',\n",
       " 'and',\n",
       " 'deriving',\n",
       " 'parameter',\n",
       " 'uncertainties',\n",
       " 'is',\n",
       " 'often',\n",
       " 'an',\n",
       " 'after',\n",
       " 'thought',\n",
       " 'classic',\n",
       " 'forward',\n",
       " 'folding',\n",
       " 'analyses',\n",
       " 'with',\n",
       " 'markov_chain',\n",
       " 'monte_carlo',\n",
       " 'or',\n",
       " 'nested',\n",
       " 'sampling',\n",
       " 'enable',\n",
       " 'parameter',\n",
       " 'estimation',\n",
       " 'and',\n",
       " 'model',\n",
       " 'comparison',\n",
       " 'even',\n",
       " 'for',\n",
       " 'complex',\n",
       " 'and',\n",
       " 'slow',\n",
       " 'to',\n",
       " 'evaluate',\n",
       " 'physical',\n",
       " 'models',\n",
       " 'however',\n",
       " 'these',\n",
       " 'approaches',\n",
       " 'require',\n",
       " 'independent',\n",
       " 'runs',\n",
       " 'for',\n",
       " 'each',\n",
       " 'data',\n",
       " 'set',\n",
       " 'implying',\n",
       " 'an',\n",
       " 'unfeasible',\n",
       " 'number_of',\n",
       " 'model',\n",
       " 'evaluations',\n",
       " 'in',\n",
       " 'the',\n",
       " 'big',\n",
       " 'data',\n",
       " 'regime',\n",
       " 'here',\n",
       " 'present',\n",
       " 'new',\n",
       " 'algorithm',\n",
       " 'collaborative',\n",
       " 'nested',\n",
       " 'sampling',\n",
       " 'for',\n",
       " 'deriving',\n",
       " 'parameter',\n",
       " 'probability_distributions',\n",
       " 'for',\n",
       " 'each',\n",
       " 'observation',\n",
       " 'importantly',\n",
       " 'the',\n",
       " 'number_of',\n",
       " 'physical',\n",
       " 'model',\n",
       " 'evaluations',\n",
       " 'scales',\n",
       " 'sub',\n",
       " 'linearly',\n",
       " 'with',\n",
       " 'the',\n",
       " 'number_of',\n",
       " 'data_sets',\n",
       " 'and',\n",
       " 'no',\n",
       " 'assumptions',\n",
       " 'about',\n",
       " 'homogeneous',\n",
       " 'errors',\n",
       " 'gaussianity',\n",
       " 'the',\n",
       " 'form',\n",
       " 'of',\n",
       " 'the',\n",
       " 'model',\n",
       " 'or',\n",
       " 'heterogeneity',\n",
       " 'completeness',\n",
       " 'of',\n",
       " 'the',\n",
       " 'observations',\n",
       " 'need',\n",
       " 'to',\n",
       " 'be',\n",
       " 'made',\n",
       " 'collaborative',\n",
       " 'nested',\n",
       " 'sampling',\n",
       " 'has',\n",
       " 'immediate',\n",
       " 'application',\n",
       " 'in',\n",
       " 'speeding',\n",
       " 'up',\n",
       " 'analyses',\n",
       " 'of',\n",
       " 'large',\n",
       " 'surveys',\n",
       " 'integral',\n",
       " 'field',\n",
       " 'unit',\n",
       " 'observations',\n",
       " 'and',\n",
       " 'monte_carlo',\n",
       " 'simulations']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 10\n",
    "bigram_finder[docs[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c9d11",
   "metadata": {},
   "source": [
    "You can also apply the model across the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "347af394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x7f40704a6550>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_finder[docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc13435a",
   "metadata": {},
   "source": [
    "The Phrases class will only look for two-word phrases, but what about three-word phrases? To look for these, we can fit another model but this time pass in the result of our first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60514e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_finder = Phrases(\n",
    "    sentences = # Fill this in\n",
    "    min_count = 25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177dbd0",
   "metadata": {},
   "source": [
    "Notice how this picks up on three word phrases and some four word phrases (\"markov_chain_monte_carlo\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "770baaef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'data',\n",
       " 'torrent',\n",
       " 'unleashed',\n",
       " 'by',\n",
       " 'current',\n",
       " 'and',\n",
       " 'upcoming',\n",
       " 'astronomical',\n",
       " 'surveys',\n",
       " 'demands',\n",
       " 'scalable',\n",
       " 'analysis',\n",
       " 'methods',\n",
       " 'many',\n",
       " 'machine_learning',\n",
       " 'approaches',\n",
       " 'scale',\n",
       " 'well',\n",
       " 'but',\n",
       " 'separating',\n",
       " 'the',\n",
       " 'instrument',\n",
       " 'measurement',\n",
       " 'from',\n",
       " 'the',\n",
       " 'physical',\n",
       " 'effects',\n",
       " 'of',\n",
       " 'interest',\n",
       " 'dealing',\n",
       " 'with',\n",
       " 'variable',\n",
       " 'errors',\n",
       " 'and',\n",
       " 'deriving',\n",
       " 'parameter',\n",
       " 'uncertainties',\n",
       " 'is',\n",
       " 'often',\n",
       " 'an',\n",
       " 'after',\n",
       " 'thought',\n",
       " 'classic',\n",
       " 'forward',\n",
       " 'folding',\n",
       " 'analyses',\n",
       " 'with',\n",
       " 'markov_chain_monte_carlo',\n",
       " 'or',\n",
       " 'nested',\n",
       " 'sampling',\n",
       " 'enable',\n",
       " 'parameter',\n",
       " 'estimation',\n",
       " 'and',\n",
       " 'model',\n",
       " 'comparison',\n",
       " 'even',\n",
       " 'for',\n",
       " 'complex',\n",
       " 'and',\n",
       " 'slow',\n",
       " 'to',\n",
       " 'evaluate',\n",
       " 'physical',\n",
       " 'models',\n",
       " 'however',\n",
       " 'these',\n",
       " 'approaches',\n",
       " 'require',\n",
       " 'independent',\n",
       " 'runs',\n",
       " 'for',\n",
       " 'each',\n",
       " 'data',\n",
       " 'set',\n",
       " 'implying',\n",
       " 'an',\n",
       " 'unfeasible',\n",
       " 'number_of',\n",
       " 'model',\n",
       " 'evaluations',\n",
       " 'in',\n",
       " 'the',\n",
       " 'big',\n",
       " 'data',\n",
       " 'regime',\n",
       " 'here',\n",
       " 'present',\n",
       " 'new',\n",
       " 'algorithm',\n",
       " 'collaborative',\n",
       " 'nested',\n",
       " 'sampling',\n",
       " 'for',\n",
       " 'deriving',\n",
       " 'parameter',\n",
       " 'probability_distributions',\n",
       " 'for',\n",
       " 'each',\n",
       " 'observation',\n",
       " 'importantly',\n",
       " 'the',\n",
       " 'number_of',\n",
       " 'physical',\n",
       " 'model',\n",
       " 'evaluations',\n",
       " 'scales',\n",
       " 'sub',\n",
       " 'linearly',\n",
       " 'with',\n",
       " 'the',\n",
       " 'number_of',\n",
       " 'data_sets',\n",
       " 'and',\n",
       " 'no',\n",
       " 'assumptions',\n",
       " 'about',\n",
       " 'homogeneous',\n",
       " 'errors',\n",
       " 'gaussianity',\n",
       " 'the',\n",
       " 'form',\n",
       " 'of',\n",
       " 'the',\n",
       " 'model',\n",
       " 'or',\n",
       " 'heterogeneity',\n",
       " 'completeness',\n",
       " 'of',\n",
       " 'the',\n",
       " 'observations',\n",
       " 'need',\n",
       " 'to_be',\n",
       " 'made',\n",
       " 'collaborative',\n",
       " 'nested',\n",
       " 'sampling',\n",
       " 'has',\n",
       " 'immediate',\n",
       " 'application',\n",
       " 'in',\n",
       " 'speeding',\n",
       " 'up',\n",
       " 'analyses',\n",
       " 'of',\n",
       " 'large',\n",
       " 'surveys',\n",
       " 'integral',\n",
       " 'field',\n",
       " 'unit',\n",
       " 'observations',\n",
       " 'and',\n",
       " 'monte_carlo',\n",
       " 'simulations']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 10\n",
    "trigram_finder[bigram_finder[docs[i]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c228552",
   "metadata": {},
   "source": [
    "We'll now take the results of applying our phrase finders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5937dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(trigram_finder[bigram_finder[docs]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56220ec7",
   "metadata": {},
   "source": [
    "**Bonus:** Modify your code so that for each document, you are keeping both the original tokens and the multi-word phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c10ec",
   "metadata": {},
   "source": [
    "Now, we need to build a [gensim Dictionary](https://radimrehurek.com/gensim/corpora/dictionary.html) from our documents. This is a class which builds a token to id map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c91160b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd227d14",
   "metadata": {},
   "source": [
    "This object can convert from tokens to ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d3a7292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'argue_that': 1,\n",
       " 'automatically': 2,\n",
       " 'burstiness': 3,\n",
       " 'conflicts': 4,\n",
       " 'contentiousness': 5,\n",
       " 'detecting': 6,\n",
       " 'deviate': 7,\n",
       " 'different': 8,\n",
       " 'discussions': 9,\n",
       " 'earlier': 10,\n",
       " 'edit': 11,\n",
       " 'editing': 12,\n",
       " 'edits': 13,\n",
       " 'efficient': 14,\n",
       " 'estimated': 15,\n",
       " 'evaluate': 16,\n",
       " 'following': 17,\n",
       " 'for': 18,\n",
       " 'from': 19,\n",
       " 'general': 20,\n",
       " 'has': 21,\n",
       " 'how': 22,\n",
       " 'in': 23,\n",
       " 'language': 24,\n",
       " 'length': 25,\n",
       " 'method': 26,\n",
       " 'new': 27,\n",
       " 'number_of': 28,\n",
       " 'of': 29,\n",
       " 'on': 30,\n",
       " 'over': 31,\n",
       " 'pages': 32,\n",
       " 'process': 33,\n",
       " 'reverts': 34,\n",
       " 'severe': 35,\n",
       " 'significantly': 36,\n",
       " 'six': 37,\n",
       " 'such': 38,\n",
       " 'the': 39,\n",
       " 'this': 40,\n",
       " 'those': 41,\n",
       " 'wars': 42,\n",
       " 'we_discuss': 43,\n",
       " 'we_present': 44,\n",
       " 'wikipedia': 45,\n",
       " 'work': 46,\n",
       " 'workflow': 47,\n",
       " 'wps': 48,\n",
       " 'ability': 49,\n",
       " 'accurate': 50,\n",
       " 'actual': 51,\n",
       " 'adaboost': 52,\n",
       " 'address': 53,\n",
       " 'affirmative': 54,\n",
       " 'algorithm': 55,\n",
       " 'algorithms': 56,\n",
       " 'alleviate': 57,\n",
       " 'almost': 58,\n",
       " 'always': 59,\n",
       " 'among': 60,\n",
       " 'an': 61,\n",
       " 'analysis': 62,\n",
       " 'answer': 63,\n",
       " 'approximations': 64,\n",
       " 'arbitrarily': 65,\n",
       " 'are': 66,\n",
       " 'arguably': 67,\n",
       " 'as': 68,\n",
       " 'at_least': 69,\n",
       " 'averages': 70,\n",
       " 'behaves': 71,\n",
       " 'behavior': 72,\n",
       " 'being': 73,\n",
       " 'believe': 74,\n",
       " 'best': 75,\n",
       " 'broad': 76,\n",
       " 'burden': 77,\n",
       " 'by': 78,\n",
       " 'certain': 79,\n",
       " 'classifier': 80,\n",
       " 'coined': 81,\n",
       " 'community': 82,\n",
       " 'computational': 83,\n",
       " 'condition': 84,\n",
       " 'conditions': 85,\n",
       " 'conjectures': 86,\n",
       " 'constructive': 87,\n",
       " 'continuous': 88,\n",
       " 'convergence': 89,\n",
       " 'convergence_rate': 90,\n",
       " 'cycles': 91,\n",
       " 'cycling': 92,\n",
       " 'data_sets': 93,\n",
       " 'daubechies': 94,\n",
       " 'deciding': 95,\n",
       " 'detect': 96,\n",
       " 'does_not': 97,\n",
       " 'dynamical_system': 98,\n",
       " 'effective': 99,\n",
       " 'elegant': 100,\n",
       " 'empirical': 101,\n",
       " 'employing': 102,\n",
       " 'ergodic': 103,\n",
       " 'establish': 104,\n",
       " 'eventually': 105,\n",
       " 'evidence': 106,\n",
       " 'exhibit': 107,\n",
       " 'existence': 108,\n",
       " 'finite': 109,\n",
       " 'fixed': 110,\n",
       " 'found': 111,\n",
       " 'frame': 112,\n",
       " 'future': 113,\n",
       " 'generalization': 114,\n",
       " 'generalization_error': 115,\n",
       " 'ground': 116,\n",
       " 'hard': 117,\n",
       " 'have': 118,\n",
       " 'help': 119,\n",
       " 'high_dimensional': 120,\n",
       " 'hold': 121,\n",
       " 'implement': 122,\n",
       " 'in_practice': 123,\n",
       " 'interesting': 124,\n",
       " 'is': 125,\n",
       " 'it': 126,\n",
       " 'its': 127,\n",
       " 'itself': 128,\n",
       " 'like': 129,\n",
       " 'long': 130,\n",
       " 'many': 131,\n",
       " 'map': 132,\n",
       " 'margins': 133,\n",
       " 'mathematically': 134,\n",
       " 'may': 135,\n",
       " 'ml': 136,\n",
       " 'most': 137,\n",
       " 'multiple': 138,\n",
       " 'objects': 139,\n",
       " 'often': 140,\n",
       " 'one': 141,\n",
       " 'open': 142,\n",
       " 'optimal': 143,\n",
       " 'optimize': 144,\n",
       " 'other': 145,\n",
       " 'our_results': 146,\n",
       " 'perspective': 147,\n",
       " 'popular': 148,\n",
       " 'practitioner': 149,\n",
       " 'practitioners': 150,\n",
       " 'proofs': 151,\n",
       " 'properties': 152,\n",
       " 'prove_that': 153,\n",
       " 'provide': 154,\n",
       " 'puzzles': 155,\n",
       " 'quickly': 156,\n",
       " 'real_world_datasets': 157,\n",
       " 'reasonable': 158,\n",
       " 'reasonably': 159,\n",
       " 'resulting': 160,\n",
       " 'rudin': 161,\n",
       " 'run': 162,\n",
       " 'same': 163,\n",
       " 'schapire': 164,\n",
       " 'sense': 165,\n",
       " 'several': 166,\n",
       " 'simple': 167,\n",
       " 'sound': 168,\n",
       " 'specifically': 169,\n",
       " 'stability': 170,\n",
       " 'stabilize': 171,\n",
       " 'still': 172,\n",
       " 'strong': 173,\n",
       " 'sufficient_conditions': 174,\n",
       " 'term': 175,\n",
       " 'that': 176,\n",
       " 'theoretically': 177,\n",
       " 'theory': 178,\n",
       " 'they': 179,\n",
       " 'ties': 180,\n",
       " 'time': 181,\n",
       " 'to': 182,\n",
       " 'tools': 183,\n",
       " 'two': 184,\n",
       " 'under': 185,\n",
       " 'universal': 186,\n",
       " 'update': 187,\n",
       " 'very': 188,\n",
       " 'we': 189,\n",
       " 'we_prove': 190,\n",
       " 'we_provide': 191,\n",
       " 'weak': 192,\n",
       " 'which': 193,\n",
       " 'while': 194,\n",
       " 'with': 195,\n",
       " 'acoustic': 196,\n",
       " 'acquire': 197,\n",
       " 'acquisition': 198,\n",
       " 'analyze': 199,\n",
       " 'analyzer': 200,\n",
       " 'articulation': 201,\n",
       " 'assuming': 202,\n",
       " 'automatic': 203,\n",
       " 'based_on': 204,\n",
       " 'baseline': 205,\n",
       " 'bayesian': 206,\n",
       " 'blocked': 207,\n",
       " 'called': 208,\n",
       " 'can': 209,\n",
       " 'carried': 210,\n",
       " 'categorization': 211,\n",
       " 'combines': 212,\n",
       " 'conventional': 213,\n",
       " 'daa': 214,\n",
       " 'data': 215,\n",
       " 'derived': 216,\n",
       " 'developed': 217,\n",
       " 'direct': 218,\n",
       " 'directly': 219,\n",
       " 'dirichlet': 220,\n",
       " 'discover': 221,\n",
       " 'double': 222,\n",
       " 'embedded': 223,\n",
       " 'enables': 224,\n",
       " 'estimate': 225,\n",
       " 'et_al': 226,\n",
       " 'evaluation': 227,\n",
       " 'experiments': 228,\n",
       " 'explicitly': 229,\n",
       " 'extending': 230,\n",
       " 'generative_model': 231,\n",
       " 'gibbs': 232,\n",
       " 'hdp': 233,\n",
       " 'hidden': 234,\n",
       " 'hierarchical': 235,\n",
       " 'hierarchically': 236,\n",
       " 'hlm': 237,\n",
       " 'hsmm': 238,\n",
       " 'human': 239,\n",
       " 'in_this_paper': 240,\n",
       " 'infants': 241,\n",
       " 'inference': 242,\n",
       " 'inferring': 243,\n",
       " 'integrative': 244,\n",
       " 'into': 245,\n",
       " 'japanese': 246,\n",
       " 'johnson': 247,\n",
       " 'labeled_data': 248,\n",
       " 'latent': 249,\n",
       " 'latent_variables': 250,\n",
       " 'machine_learning': 251,\n",
       " 'manner': 252,\n",
       " 'markov': 253,\n",
       " 'model': 254,\n",
       " 'models': 255,\n",
       " 'nonparametric': 256,\n",
       " 'novel': 257,\n",
       " 'npb': 258,\n",
       " 'observed': 259,\n",
       " 'obtained': 260,\n",
       " 'organized': 261,\n",
       " 'originally': 262,\n",
       " 'out': 263,\n",
       " 'outperformed': 264,\n",
       " 'phoneme': 265,\n",
       " 'phonemes': 266,\n",
       " 'procedure': 267,\n",
       " 'proposed': 268,\n",
       " 'purpose': 269,\n",
       " 'recognition': 270,\n",
       " 'representing': 271,\n",
       " 'sampler': 272,\n",
       " 'semi': 273,\n",
       " 'sequences': 274,\n",
       " 'signals': 275,\n",
       " 'simultaneous': 276,\n",
       " 'single': 277,\n",
       " 'speech': 278,\n",
       " 'structure': 279,\n",
       " 'supervised': 280,\n",
       " 'synthetic_data': 281,\n",
       " 'system': 282,\n",
       " 'tasks': 283,\n",
       " 'time_series': 284,\n",
       " 'trained': 285,\n",
       " 'unsegmented': 286,\n",
       " 'unsupervised': 287,\n",
       " 'using': 288,\n",
       " 'vowel': 289,\n",
       " 'was': 290,\n",
       " 'we_also': 291,\n",
       " 'we_develop': 292,\n",
       " 'we_propose': 293,\n",
       " 'whose': 294,\n",
       " 'without_any': 295,\n",
       " 'word': 296,\n",
       " 'words': 297,\n",
       " 'accurately': 298,\n",
       " 'actions': 299,\n",
       " 'active': 300,\n",
       " 'after': 301,\n",
       " 'allow': 302,\n",
       " 'an_efficient': 303,\n",
       " 'approximation': 304,\n",
       " 'auditory': 305,\n",
       " 'based': 306,\n",
       " 'because': 307,\n",
       " 'between': 308,\n",
       " 'calculation': 309,\n",
       " 'can_be': 310,\n",
       " 'categories': 311,\n",
       " 'conducted': 312,\n",
       " 'criterion': 313,\n",
       " 'decreasing': 314,\n",
       " 'determine': 315,\n",
       " 'divergence': 316,\n",
       " 'equivalent': 317,\n",
       " 'expected': 318,\n",
       " 'experiment': 319,\n",
       " 'experimental_results': 320,\n",
       " 'final': 321,\n",
       " 'form': 322,\n",
       " 'function': 323,\n",
       " 'gain': 324,\n",
       " 'graphical': 325,\n",
       " 'greedy': 326,\n",
       " 'haptic': 327,\n",
       " 'however': 328,\n",
       " 'humanoid': 329,\n",
       " 'ig': 330,\n",
       " 'impossible': 331,\n",
       " 'information': 332,\n",
       " 'justification': 333,\n",
       " 'kullback_leibler': 334,\n",
       " 'lazy': 335,\n",
       " 'limited': 336,\n",
       " 'making': 337,\n",
       " 'maximization': 338,\n",
       " 'means': 339,\n",
       " 'mhdp': 340,\n",
       " 'minimization': 341,\n",
       " 'monte_carlo': 342,\n",
       " 'multimodal': 343,\n",
       " 'next': 344,\n",
       " 'non': 345,\n",
       " 'object': 346,\n",
       " 'our_theoretical': 347,\n",
       " 'outcomes': 348,\n",
       " 'perception': 349,\n",
       " 'performance': 350,\n",
       " 'performing': 351,\n",
       " 'practically': 352,\n",
       " 'problem': 353,\n",
       " 'property': 354,\n",
       " 'real': 355,\n",
       " 'recognize': 356,\n",
       " 'recognizing': 357,\n",
       " 'reduced': 358,\n",
       " 'requires': 359,\n",
       " 'results': 360,\n",
       " 'robot': 361,\n",
       " 'scenario': 362,\n",
       " 'second': 363,\n",
       " 'select': 364,\n",
       " 'set': 365,\n",
       " 'show_that': 366,\n",
       " 'state': 367,\n",
       " 'straightforward': 368,\n",
       " 'submodular': 369,\n",
       " 'support': 370,\n",
       " 'target': 371,\n",
       " 'their': 372,\n",
       " 'theoretical': 373,\n",
       " 'therefore': 374,\n",
       " 'torso': 375,\n",
       " 'upper': 376,\n",
       " 'use': 377,\n",
       " 'uses': 378,\n",
       " 'visual': 379,\n",
       " 'we_derive': 380,\n",
       " 'we_show_that': 381,\n",
       " 'when': 382,\n",
       " 'adaptation': 383,\n",
       " 'cma': 384,\n",
       " 'concepts': 385,\n",
       " 'covariance_matrix': 386,\n",
       " 'derive': 387,\n",
       " 'domain': 388,\n",
       " 'es': 389,\n",
       " 'evolution': 390,\n",
       " 'functions': 391,\n",
       " 'introduces': 392,\n",
       " 'intuitive': 393,\n",
       " 'motivate': 394,\n",
       " 'non_convex': 395,\n",
       " 'non_linear': 396,\n",
       " 'optimization': 397,\n",
       " 'or': 398,\n",
       " 'parameter': 399,\n",
       " 'randomized': 400,\n",
       " 'requirements': 401,\n",
       " 'search': 402,\n",
       " 'stands': 403,\n",
       " 'stochastic': 404,\n",
       " 'strategy': 405,\n",
       " 'try': 406,\n",
       " 'tutorial': 407,\n",
       " 'where': 408,\n",
       " 'adaptive': 409,\n",
       " 'aggregate': 410,\n",
       " 'allows_us_to': 411,\n",
       " 'amount': 412,\n",
       " 'analyzing': 413,\n",
       " 'any': 414,\n",
       " 'appropriate': 415,\n",
       " 'arm': 416,\n",
       " 'art': 417,\n",
       " 'asymptotic': 418,\n",
       " 'bad': 419,\n",
       " 'bandit_problem': 420,\n",
       " 'best_arm': 421,\n",
       " 'both': 422,\n",
       " 'but': 423,\n",
       " 'change': 424,\n",
       " 'collected': 425,\n",
       " 'concreteness': 426,\n",
       " 'confidence': 427,\n",
       " 'considering': 428,\n",
       " 'constraints': 429,\n",
       " 'could_be': 430,\n",
       " 'delta': 431,\n",
       " 'differs': 432,\n",
       " 'difficult': 433,\n",
       " 'distinguish': 434,\n",
       " 'drowned': 435,\n",
       " 'each': 436,\n",
       " 'either': 437,\n",
       " 'em': 438,\n",
       " 'emph': 439,\n",
       " 'existing_methods': 440,\n",
       " 'exploration': 441,\n",
       " 'extraneous': 442,\n",
       " 'factors': 443,\n",
       " 'fano': 444,\n",
       " 'first': 445,\n",
       " 'gap_between': 446,\n",
       " 'gathered': 447,\n",
       " 'given': 448,\n",
       " 'good': 449,\n",
       " 'identification': 450,\n",
       " 'imply': 451,\n",
       " 'incorporate': 452,\n",
       " 'individual': 453,\n",
       " 'inspires': 454,\n",
       " 'instance': 455,\n",
       " 'kind': 456,\n",
       " 'latter': 457,\n",
       " 'limitations': 458,\n",
       " 'literature': 459,\n",
       " 'log': 460,\n",
       " 'lower_bounds': 461,\n",
       " 'match': 462,\n",
       " 'measure': 463,\n",
       " 'moderate': 464,\n",
       " 'moreover': 465,\n",
       " 'much': 466,\n",
       " 'multi': 467,\n",
       " 'near_optimal': 468,\n",
       " 'needs': 469,\n",
       " 'not': 470,\n",
       " 'our': 471,\n",
       " 'our_approach': 472,\n",
       " 'outperforms': 473,\n",
       " 'phenomena': 474,\n",
       " 'practical': 475,\n",
       " 'prove': 476,\n",
       " 'pulled': 477,\n",
       " 'pure': 478,\n",
       " 'removes': 479,\n",
       " 'sample_complexity': 480,\n",
       " 'sampling': 481,\n",
       " 'setting_where': 482,\n",
       " 'simulator': 483,\n",
       " 'strength': 484,\n",
       " 'structured': 485,\n",
       " 'substantial': 486,\n",
       " 'succumbing': 487,\n",
       " 'technique': 488,\n",
       " 'techniques': 489,\n",
       " 'times': 490,\n",
       " 'to_be': 491,\n",
       " 'top': 492,\n",
       " 'uncovering': 493,\n",
       " 'up_to': 494,\n",
       " 'we_apply': 495,\n",
       " 'we_propose_novel': 496,\n",
       " 'without': 497,\n",
       " 'zero': 498,\n",
       " 'achieving': 499,\n",
       " 'aims': 500,\n",
       " 'approaches': 501,\n",
       " 'arms': 502,\n",
       " 'at': 503,\n",
       " 'bandit': 504,\n",
       " 'cannot': 505,\n",
       " 'challenges': 506,\n",
       " 'characteristics': 507,\n",
       " 'classical': 508,\n",
       " 'contextual': 509,\n",
       " 'depending_on': 510,\n",
       " 'do_not': 511,\n",
       " 'entries': 512,\n",
       " 'far': 513,\n",
       " 'feature': 514,\n",
       " 'features': 515,\n",
       " 'finds': 516,\n",
       " 'guarantee': 517,\n",
       " 'highly': 518,\n",
       " 'hypothesis': 519,\n",
       " 'implies': 520,\n",
       " 'large_number': 521,\n",
       " 'linear': 522,\n",
       " 'missing': 523,\n",
       " 'noise': 524,\n",
       " 'noiseless': 525,\n",
       " 'noisy': 526,\n",
       " 'occur': 527,\n",
       " 'oracle': 528,\n",
       " 'oracles': 529,\n",
       " 'problems': 530,\n",
       " 'realizability': 531,\n",
       " 'regret_bound': 532,\n",
       " 'setups': 533,\n",
       " 'synthetic': 534,\n",
       " 'there': 535,\n",
       " 'tilde_sqrt': 536,\n",
       " 'trivial': 537,\n",
       " 'uncertainty': 538,\n",
       " 'underlying': 539,\n",
       " 'we_demonstrate': 540,\n",
       " 'we_study': 541,\n",
       " 'aims_to': 542,\n",
       " 'applications': 543,\n",
       " 'approach': 544,\n",
       " 'approximate': 545,\n",
       " 'basic': 546,\n",
       " 'basis': 547,\n",
       " 'complex': 548,\n",
       " 'computing': 549,\n",
       " 'decision': 550,\n",
       " 'develops': 551,\n",
       " 'dramatically': 552,\n",
       " 'emerged_as': 553,\n",
       " 'ensemble': 554,\n",
       " 'even': 555,\n",
       " 'expands': 556,\n",
       " 'face': 557,\n",
       " 'further': 558,\n",
       " 'heuristic': 559,\n",
       " 'insight': 560,\n",
       " 'neural_networks': 561,\n",
       " 'offer': 562,\n",
       " 'online': 563,\n",
       " 'only': 564,\n",
       " 'posterior_distribution': 565,\n",
       " 'present': 566,\n",
       " 'range': 567,\n",
       " 'special_cases': 568,\n",
       " 'such_as': 569,\n",
       " 'supports': 570,\n",
       " 'this_paper': 571,\n",
       " 'thompson_sampling': 572,\n",
       " 'tractability': 573,\n",
       " 'tractable': 574,\n",
       " 'viable': 575,\n",
       " 'we_establish': 576,\n",
       " 'while_maintaining': 577,\n",
       " 'as_well_as': 578,\n",
       " 'aspects': 579,\n",
       " 'avoid': 580,\n",
       " 'bayesian_inference': 581,\n",
       " 'blockmodel': 582,\n",
       " 'can_be_used': 583,\n",
       " 'chapter': 584,\n",
       " 'choice': 585,\n",
       " 'contained': 586,\n",
       " 'contrast': 587,\n",
       " 'corrected': 588,\n",
       " 'degree': 589,\n",
       " 'describing': 590,\n",
       " 'detectability': 591,\n",
       " 'extract': 592,\n",
       " 'finding': 593,\n",
       " 'formulations': 594,\n",
       " 'fundamental': 595,\n",
       " 'generalizations': 596,\n",
       " 'hierarchies': 597,\n",
       " 'in_particular': 598,\n",
       " 'increased': 599,\n",
       " 'introduction': 600,\n",
       " 'large_scale': 601,\n",
       " 'light': 602,\n",
       " 'links': 603,\n",
       " 'maximizes': 604,\n",
       " 'model_selection': 605,\n",
       " 'modular': 606,\n",
       " 'network': 607,\n",
       " 'networks': 608,\n",
       " 'overfitting': 609,\n",
       " 'overlapping': 610,\n",
       " 'partitions': 611,\n",
       " 'perform': 612,\n",
       " 'point': 613,\n",
       " 'predict': 614,\n",
       " 'prevents': 615,\n",
       " 'priors': 616,\n",
       " 'provides': 617,\n",
       " 'sbm': 618,\n",
       " 'self': 619,\n",
       " 'shed': 620,\n",
       " 'show_how': 621,\n",
       " 'spurious': 622,\n",
       " 'structures': 623,\n",
       " 'task': 624,\n",
       " 'underfitting': 625,\n",
       " 'via': 626,\n",
       " 'we_focus_on': 627,\n",
       " 'about': 628,\n",
       " 'access_to': 629,\n",
       " 'agent': 630,\n",
       " 'applied': 631,\n",
       " 'atari': 632,\n",
       " 'behaviors': 633,\n",
       " 'communicate': 634,\n",
       " 'considerably': 635,\n",
       " 'cost': 636,\n",
       " 'defined': 637,\n",
       " 'demonstrate': 638,\n",
       " 'effectively': 639,\n",
       " 'enough': 640,\n",
       " 'environment': 641,\n",
       " 'environments': 642,\n",
       " 'expert': 643,\n",
       " 'explore': 644,\n",
       " 'feedback': 645,\n",
       " 'flexibility': 646,\n",
       " 'games': 647,\n",
       " 'goals': 648,\n",
       " 'have_been': 649,\n",
       " 'hour': 650,\n",
       " 'in_terms_of': 651,\n",
       " 'in_this_work': 652,\n",
       " 'including': 653,\n",
       " 'interact': 654,\n",
       " 'interactions': 655,\n",
       " 'learned': 656,\n",
       " 'less': 657,\n",
       " 'locomotion': 658,\n",
       " 'more': 659,\n",
       " 'need': 660,\n",
       " 'oversight': 661,\n",
       " 'pairs': 662,\n",
       " 'percent': 663,\n",
       " 'preferences': 664,\n",
       " 'previously': 665,\n",
       " 'providing': 666,\n",
       " 'real_world': 667,\n",
       " 'reduces': 668,\n",
       " 'reinforcement_learning_rl': 669,\n",
       " 'reward': 670,\n",
       " 'rl': 671,\n",
       " 'segments': 672,\n",
       " 'simulated': 673,\n",
       " 'solve': 674,\n",
       " 'sophisticated': 675,\n",
       " 'successfully': 676,\n",
       " 'systems': 677,\n",
       " 'than': 678,\n",
       " 'these': 679,\n",
       " 'train': 680,\n",
       " 'trajectory': 681,\n",
       " 'usefully': 682,\n",
       " 'analyses': 683,\n",
       " 'application': 684,\n",
       " 'assumptions': 685,\n",
       " 'astronomical': 686,\n",
       " 'big': 687,\n",
       " 'classic': 688,\n",
       " 'collaborative': 689,\n",
       " 'comparison': 690,\n",
       " 'completeness': 691,\n",
       " 'current': 692,\n",
       " 'dealing': 693,\n",
       " 'demands': 694,\n",
       " 'deriving': 695,\n",
       " 'effects': 696,\n",
       " 'enable': 697,\n",
       " 'errors': 698,\n",
       " 'estimation': 699,\n",
       " 'evaluations': 700,\n",
       " 'field': 701,\n",
       " 'folding': 702,\n",
       " 'forward': 703,\n",
       " 'gaussianity': 704,\n",
       " 'here': 705,\n",
       " 'heterogeneity': 706,\n",
       " 'homogeneous': 707,\n",
       " 'immediate': 708,\n",
       " 'implying': 709,\n",
       " 'importantly': 710,\n",
       " 'independent': 711,\n",
       " 'instrument': 712,\n",
       " 'integral': 713,\n",
       " 'interest': 714,\n",
       " 'large': 715,\n",
       " 'linearly': 716,\n",
       " 'made': 717,\n",
       " 'markov_chain_monte_carlo': 718,\n",
       " 'measurement': 719,\n",
       " 'methods': 720,\n",
       " 'nested': 721,\n",
       " 'no': 722,\n",
       " 'observation': 723,\n",
       " 'observations': 724,\n",
       " 'physical': 725,\n",
       " 'probability_distributions': 726,\n",
       " 'regime': 727,\n",
       " 'require': 728,\n",
       " 'runs': 729,\n",
       " 'scalable': 730,\n",
       " 'scale': 731,\n",
       " 'scales': 732,\n",
       " 'separating': 733,\n",
       " 'simulations': 734,\n",
       " 'slow': 735,\n",
       " 'speeding': 736,\n",
       " 'sub': 737,\n",
       " 'surveys': 738,\n",
       " 'thought': 739,\n",
       " 'torrent': 740,\n",
       " 'uncertainties': 741,\n",
       " 'unfeasible': 742,\n",
       " 'unit': 743,\n",
       " 'unleashed': 744,\n",
       " 'up': 745,\n",
       " 'upcoming': 746,\n",
       " 'variable': 747,\n",
       " 'well': 748,\n",
       " 'accuracy': 749,\n",
       " 'additional': 750,\n",
       " 'advantages': 751,\n",
       " 'all': 752,\n",
       " 'also': 753,\n",
       " 'applicability': 754,\n",
       " 'boundary': 755,\n",
       " 'build': 756,\n",
       " 'built': 757,\n",
       " 'classes': 758,\n",
       " 'classification': 759,\n",
       " 'classifiers': 760,\n",
       " 'component': 761,\n",
       " 'conditional': 762,\n",
       " 'continuum': 763,\n",
       " 'datasets': 764,\n",
       " 'difference': 765,\n",
       " 'ease': 766,\n",
       " 'especially': 767,\n",
       " 'existing': 768,\n",
       " 'family': 769,\n",
       " 'fast': 770,\n",
       " 'here_we': 771,\n",
       " 'high': 772,\n",
       " 'if': 773,\n",
       " 'improve': 774,\n",
       " 'improving': 775,\n",
       " 'individually': 776,\n",
       " 'interpretation': 777,\n",
       " 'kernel': 778,\n",
       " 'machines': 779,\n",
       " 'magnitude': 780,\n",
       " 'opposite': 781,\n",
       " 'order': 782,\n",
       " 'orders': 783,\n",
       " 'piecewise': 784,\n",
       " 'portion': 785,\n",
       " 'powerful': 786,\n",
       " 'probabilities': 787,\n",
       " 'probability': 788,\n",
       " 'rather_than': 789,\n",
       " 'represent': 790,\n",
       " 'representation': 791,\n",
       " 'root': 792,\n",
       " 'significant': 793,\n",
       " 'similarly': 794,\n",
       " 'simplicity': 795,\n",
       " 'smooth': 796,\n",
       " 'speed': 797,\n",
       " 'statistical': 798,\n",
       " 'succeeded': 799,\n",
       " 'suited': 800,\n",
       " 'support_vector': 801,\n",
       " 'svm': 802,\n",
       " 'tested': 803,\n",
       " 'them': 804,\n",
       " 'through': 805,\n",
       " 'too': 806,\n",
       " 'training': 807,\n",
       " 'versatile': 808,\n",
       " 'were': 809,\n",
       " 'works': 810,\n",
       " 'yet': 811,\n",
       " 'addressed': 812,\n",
       " 'alzheimer': 813,\n",
       " 'arabidopsis': 814,\n",
       " 'associations': 815,\n",
       " 'better_than': 816,\n",
       " 'capable': 817,\n",
       " 'causal': 818,\n",
       " 'clusters': 819,\n",
       " 'co': 820,\n",
       " 'competitive': 821,\n",
       " 'confounding': 822,\n",
       " 'correcting': 823,\n",
       " 'correction': 824,\n",
       " 'correlation': 825,\n",
       " 'crucial': 826,\n",
       " 'cryptic': 827,\n",
       " 'dataset': 828,\n",
       " 'discovered': 829,\n",
       " 'discuss': 830,\n",
       " 'disease': 831,\n",
       " 'effectiveness': 832,\n",
       " 'example': 833,\n",
       " 'existing_approaches': 834,\n",
       " 'expressed': 835,\n",
       " 'extensive': 836,\n",
       " 'genetic': 837,\n",
       " 'geneticists': 838,\n",
       " 'genomic': 839,\n",
       " 'genotypic': 840,\n",
       " 'graph': 841,\n",
       " 'has_shown': 842,\n",
       " 'hence': 843,\n",
       " 'heterogeneous': 844,\n",
       " 'humans': 845,\n",
       " 'important': 846,\n",
       " 'joint': 847,\n",
       " 'justify': 848,\n",
       " 'lmm': 849,\n",
       " 'loci': 850,\n",
       " 'mixed': 851,\n",
       " 'modeling': 852,\n",
       " 'others': 853,\n",
       " 'our_method': 854,\n",
       " 'phenotypes': 855,\n",
       " 'phenotypic': 856,\n",
       " 'plants': 857,\n",
       " 'population': 858,\n",
       " 'potential': 859,\n",
       " 'raised': 860,\n",
       " 'regarding': 861,\n",
       " 'relatedness': 862,\n",
       " 'sglmm': 863,\n",
       " 'shared': 864,\n",
       " 'simulation': 865,\n",
       " 'some': 866,\n",
       " 'sparse': 867,\n",
       " 'species': 868,\n",
       " 'stratification': 869,\n",
       " 'thaliana': 870,\n",
       " 'together': 871,\n",
       " 'traits': 872,\n",
       " 'utilize': 873,\n",
       " 'validate': 874,\n",
       " 'variation': 875,\n",
       " 'activation': 876,\n",
       " 'anisotropy': 877,\n",
       " 'calculate': 878,\n",
       " 'case': 879,\n",
       " 'compared_to': 880,\n",
       " 'compute': 881,\n",
       " 'constant': 882,\n",
       " 'detection': 883,\n",
       " 'fraction': 884,\n",
       " 'functional': 885,\n",
       " 'illustrated': 886,\n",
       " 'imaging': 887,\n",
       " 'increases': 888,\n",
       " 'indicate': 889,\n",
       " 'interaction': 890,\n",
       " 'intractable': 891,\n",
       " 'ising': 892,\n",
       " 'likelihood': 893,\n",
       " 'magnetic': 894,\n",
       " 'make': 895,\n",
       " 'mean': 896,\n",
       " 'normalizing': 897,\n",
       " 'numerically': 898,\n",
       " 'patterns': 899,\n",
       " 'pistachio': 900,\n",
       " 'possible': 901,\n",
       " 'quantities': 902,\n",
       " 'ratio': 903,\n",
       " 'resonance': 904,\n",
       " 'simulation_studies': 905,\n",
       " 'spatial': 906,\n",
       " 'spin': 907,\n",
       " 'taken': 908,\n",
       " 'testing': 909,\n",
       " 'tiny': 910,\n",
       " 'tree': 911,\n",
       " 'value': 912,\n",
       " 'vertices': 913,\n",
       " 'yearly': 914,\n",
       " 'yields': 915,\n",
       " 'achieves': 916,\n",
       " 'autoencoder': 917,\n",
       " 'been': 918,\n",
       " 'capture': 919,\n",
       " 'conjunction': 920,\n",
       " 'deep': 921,\n",
       " 'demonstrated': 922,\n",
       " 'directed': 923,\n",
       " 'elbo': 924,\n",
       " 'embedding': 925,\n",
       " 'encoding': 926,\n",
       " 'fail': 927,\n",
       " 'fine': 928,\n",
       " 'framework': 929,\n",
       " 'grained': 930,\n",
       " 'has_been': 931,\n",
       " 'incorporating': 932,\n",
       " 'integrated': 933,\n",
       " 'kingma': 934,\n",
       " 'label': 935,\n",
       " 'learn': 936,\n",
       " 'learning': 937,\n",
       " 'loss': 938,\n",
       " 'lower_bound': 939,\n",
       " 'metric': 940,\n",
       " 'mnist': 941,\n",
       " 'optimizing': 942,\n",
       " 'probabilistic': 943,\n",
       " 'project': 944,\n",
       " 'proved': 945,\n",
       " 'relying_on': 946,\n",
       " 'representation_learning': 947,\n",
       " 'salient': 948,\n",
       " 'semantic': 949,\n",
       " 'similarity': 950,\n",
       " 'standard': 951,\n",
       " 'traditional': 952,\n",
       " 'triplet': 953,\n",
       " 'tvae': 954,\n",
       " 'used': 955,\n",
       " 'vae': 956,\n",
       " 'variational': 957,\n",
       " 'vectors': 958,\n",
       " 'we_call': 959,\n",
       " 'welling': 960,\n",
       " 'widely': 961,\n",
       " 'bound': 962,\n",
       " 'davis': 963,\n",
       " 'describes': 964,\n",
       " 'gaussian': 965,\n",
       " 'independent_interest': 966,\n",
       " 'kahan': 967,\n",
       " 'key': 968,\n",
       " 'matrix': 969,\n",
       " 'may_be': 970,\n",
       " 'obtain': 971,\n",
       " 'perturbation': 972,\n",
       " 'random': 973,\n",
       " 'result': 974,\n",
       " 'sharp': 975,\n",
       " 'significantly_improves': 976,\n",
       " 'sin': 977,\n",
       " 'singular': 978,\n",
       " 'small': 979,\n",
       " 'structural': 980,\n",
       " 'subjected': 981,\n",
       " 'subspaces': 982,\n",
       " 'theorem': 983,\n",
       " 'theta': 984,\n",
       " 'under_certain': 985,\n",
       " 'upon': 986,\n",
       " 'values': 987,\n",
       " 'version': 988,\n",
       " 'wedin': 989,\n",
       " 'worst_case': 990,\n",
       " 'accounting': 991,\n",
       " 'aggregating': 992,\n",
       " 'alternating': 993,\n",
       " 'areas': 994,\n",
       " 'associated': 995,\n",
       " 'attention': 996,\n",
       " 'better': 997,\n",
       " 'biology': 998,\n",
       " 'challenge': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd09404",
   "metadata": {},
   "source": [
    "To convert from id to token, you simply pass the id like you would with a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ea3e2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'burstiness'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65593965",
   "metadata": {},
   "source": [
    "The Dictionary class has some useful methods. For example, use the [filter_extremes method](https://radimrehurek.com/gensim/corpora/dictionary.html) to remove any tokens that appear in less than 20 documents or in more than 50% of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7e336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5f1a93",
   "metadata": {},
   "source": [
    "We can convert a document into a bag-of-words representation using the [doc2bow method](https://radimrehurek.com/gensim/corpora/dictionary.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7ff29b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 2),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 1),\n",
       " (24, 1),\n",
       " (25, 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.doc2bow(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915c7295",
   "metadata": {},
   "source": [
    "**Question:** This returns a list of two-element tuples. What is the meaning of the first part of each tuple? What is the meaning of the second part?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4cdd8a",
   "metadata": {},
   "source": [
    "Next, convert your documents into a bag-of-words representation and save as an object named `corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a22af722",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3589070076.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[21], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    corpus = # fill this in\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "corpus = # fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff18ba4f",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b634cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c1bb41",
   "metadata": {},
   "source": [
    "You can read more about the Gensim implementation of the LDA model here: https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "You can leave the parameters as they are set (or experiment and see how the results change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e12197d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 8            # The number of topics to be extracted\n",
    "passes = 20               # The number of times to pass through the entire corpus\n",
    "chunksize = 2000          # The number of documents to be used in a training chunk \n",
    "iterations = 400          # The maximum number of iterations through the corpus when inferring the topic distribution\n",
    "\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token     # We need to give the model the id2token dictionary\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus = corpus,\n",
    "    id2word = id2word,\n",
    "    num_topics = num_topics,\n",
    "    passes = passes,\n",
    "    chunksize = chunksize,\n",
    "    iterations = iterations,\n",
    "    alpha='auto',         # Learn an asymmetric prior for document-topic distribution from the corpus\n",
    "    eta='auto',           # Learn an asymmetric prior for topic-word distribution from the corpus\n",
    "    eval_every = None,    # Speeds up training\n",
    "    random_state = 321\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be937a0",
   "metadata": {},
   "source": [
    "Once the model has been fit, we can create a visualization of it using the pyLDAvis library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f666a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = gensimvis.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.save_html(vis, 'lda.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799361ca",
   "metadata": {},
   "source": [
    "Open up the html file that was created in your web browser and explore the topics that were found.\n",
    "\n",
    "**Question:** How does the relevance metric change as the parameter lambda goes from 0 to 1?\n",
    "\n",
    "**Question:** Look at the topic labeled as topic 6 in the visualization. What do papers related to this topic seem to be about?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4876b137",
   "metadata": {},
   "source": [
    "Once our model is fit, we can get the topic distribution for each document. Take a look at the topic distribution for the document with id 100. Does this topic distribution look reasonable, given the visualization?\n",
    "\n",
    "**Warning:** The pyldavis library starts counting at 1, whereas the gensim library starts counting at 0, so topic 1 in the html document really corresponds to topic 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17c9637c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract: using deep neural networks for identifying physics objects at the large hadron collider (lhc) has become a powerful alternative approach in recent years. after successful training of deep neural networks, examining the trained networks not only helps us understand the behaviour of neural networks, but also helps improve the performance of deep learning models through proper interpretation. we take jet tagging problem at the lhc as an example, using recursive neural networks as a starting point, aim at a thorough understanding of the behaviour of the physics-oriented dnns and the information encoded in the embedding space. we make a comparative study on a series of different jet tagging tasks dominated by different underlying physics. interesting observations on the latent space are obtained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3, 0.1322086), (5, 0.46441787), (7, 0.3985871)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 100\n",
    "\n",
    "print(f'Abstract: {papers.loc[i, \"abstract\"]}')\n",
    "model.get_document_topics(corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b732582",
   "metadata": {},
   "source": [
    "Now, build a DataFrame which has, for each document, the topic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "088d5883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4f28d1",
   "metadata": {},
   "source": [
    "Find a paper that has the highest makeup of topic 5. Then look at the abstract of this paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05c2d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b1d4ab",
   "metadata": {},
   "source": [
    "**Challenge Question:** Pick two topics and find a paper which is made up of about 50% of each of those topics. Hint: You could use the cosine similarity to find such a paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4832160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "232c8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
