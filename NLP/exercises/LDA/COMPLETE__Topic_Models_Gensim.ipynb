{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af6e646",
   "metadata": {},
   "source": [
    "## Building Topic Models with the Gensim Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6574edc7",
   "metadata": {},
   "source": [
    "For this notebook, we'll see how to fit different types of topic models using the gensim library. We'll be visualizing the results of our Latent Dirichlet Algorithm, so we'll need to install the pyLDAvis library, which we can do from conda-forge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15fa856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%conda install -c conda-forge pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8affe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import gensim\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f4ab64",
   "metadata": {},
   "source": [
    "For this notebook, we'll be using abstracts from all machine learning papers posted on arxiv.org since the beginning of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8eb3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = pd.read_csv('ml_papers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e6fce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839adb14",
   "metadata": {},
   "source": [
    "You can change the index number to preview some of the paper abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42492bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "\n",
    "print(f'Title: {papers.loc[i, \"title\"]}')\n",
    "print('----------')\n",
    "print(f'Abstract: {papers.loc[i, \"abstract\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93aafcf",
   "metadata": {},
   "source": [
    "Before applying any of these documents, we'll need to prepare the documents by preprocessing and tokenizing. For this notebook, we'll use the [simple_preprocess](https://tedboy.github.io/nlps/generated/generated/gensim.utils.simple_preprocess.html) function from the gensim library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c9eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f16029",
   "metadata": {},
   "source": [
    "Use the simple_simple function to convert the paper abstracts into a list of list of tokens named `docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64da95c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### REMOVE\n",
    "docs = list(map(simple_preprocess, papers['abstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7a4d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = # fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b1ccad",
   "metadata": {},
   "source": [
    "It's possible that the single tokens that the simple_preprocess function produces will be missing out on some possibly important phrases such as \"machine learning\" or \"convolutional neural network\". We can utilize another tool from gensim to try and automatically uncover such phrases from the text, the [Phrases](https://radimrehurek.com/gensim/models/phrases.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd88d10",
   "metadata": {},
   "source": [
    "To fit this model, we need to pass in our tokenized documents as the `sentences` argument. We can also specify other hyperparameters. Here, we'll set the minimum count to be 25, meaning these phrases must appear at least 25 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### REMOVE\n",
    "bigram_finder = Phrases(sentences = docs, min_count = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae2bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_finder = Phrases(\n",
    "    sentences = # Fill This in\n",
    "    min_count = 25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b976cdc6",
   "metadata": {},
   "source": [
    "Once the model has been fit, we can apply it to a document by passing in the document (as a list of tokens) inside a set of square brackets. Notice that the individual tokens are still present, but two-word phrases are now also listed with the two words separated by an underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ed3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "bigram_finder[docs[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c9d11",
   "metadata": {},
   "source": [
    "You can also apply the model across the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347af394",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_finder[docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc13435a",
   "metadata": {},
   "source": [
    "The Phrases class will only look for two-word phrases, but what about three-word phrases? To look for these, we can fit another model but this time pass in the result of our first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d5d178",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_finder = Phrases(\n",
    "    sentences = bigram_finder[docs],\n",
    "    min_count = 25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60514e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_finder = Phrases(\n",
    "    sentences = # Fill this in\n",
    "    min_count = 25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177dbd0",
   "metadata": {},
   "source": [
    "Notice how this picks up on three word phrases and some four word phrases (\"markov_chain_monte_carlo\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770baaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "trigram_finder[bigram_finder[docs[i]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c228552",
   "metadata": {},
   "source": [
    "We'll now take the results of applying our phrase finders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5937dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(trigram_finder[bigram_finder[docs]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56220ec7",
   "metadata": {},
   "source": [
    "**Bonus:** Modify your code so that for each document, you are keeping both the original tokens and the multi-word phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11da791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### REMOVE\n",
    "def add_bigrams_and_trigrams(document):\n",
    "    doc_bigrams = bigram_finder[document]\n",
    "    doc_trigrams = trigram_finder[bigram_finder[document]]\n",
    "    new = document + list(filter(lambda x: '_' in x, doc_bigrams))\n",
    "    new = new + list(filter(lambda x: x.count('_') == 2, doc_trigrams))\n",
    "    return new\n",
    "\n",
    "docs = list(map(add_bigrams_and_trigrams, docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c10ec",
   "metadata": {},
   "source": [
    "Now, we need to build a [gensim Dictionary](https://radimrehurek.com/gensim/corpora/dictionary.html) from our documents. This is a class which builds a token to id map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91160b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd227d14",
   "metadata": {},
   "source": [
    "This object can convert from tokens to ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3a7292",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd09404",
   "metadata": {},
   "source": [
    "To convert from id to token, you simply pass the id like you would with a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea3e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65593965",
   "metadata": {},
   "source": [
    "The Dictionary class has some useful methods. For example, use the [filter_extremes method](https://radimrehurek.com/gensim/corpora/dictionary.html) to remove any tokens that appear in less than 20 documents or in more than 50% of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###REMOVE\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7e336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5f1a93",
   "metadata": {},
   "source": [
    "We can convert a document into a bag-of-words representation using the [doc2bow method](https://radimrehurek.com/gensim/corpora/dictionary.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff29b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.doc2bow(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915c7295",
   "metadata": {},
   "source": [
    "**Question:** This returns a list of two-element tuples. What is the meaning of the first part of each tuple? What is the meaning of the second part?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4cdd8a",
   "metadata": {},
   "source": [
    "Next, convert your documents into a bag-of-words representation and save as an object named `corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffce794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### REMOVE\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22af722",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = # fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff18ba4f",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b634cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c1bb41",
   "metadata": {},
   "source": [
    "You can read more about the Gensim implementation of the LDA model here: https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "You can leave the parameters as they are set (or experiment and see how the results change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12197d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 8            # The number of topics to be extracted\n",
    "passes = 20               # The number of times to pass through the entire corpus\n",
    "chunksize = 2000          # The number of documents to be used in a training chunk \n",
    "iterations = 400          # The maximum number of iterations through the corpus when inferring the topic distribution\n",
    "\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token     # We need to give the model the id2token dictionary\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus = corpus,\n",
    "    id2word = id2word,\n",
    "    num_topics = num_topics,\n",
    "    passes = passes,\n",
    "    chunksize = chunksize,\n",
    "    iterations = iterations,\n",
    "    alpha='auto',         # Learn an asymmetric prior for document-topic distribution from the corpus\n",
    "    eta='auto',           # Learn an asymmetric prior for topic-word distribution from the corpus\n",
    "    eval_every = None,    # Speeds up training\n",
    "    random_state = 321\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be937a0",
   "metadata": {},
   "source": [
    "Once the model has been fit, we can create a visualization of it using the pyLDAvis library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f666a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = gensimvis.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.save_html(vis, 'lda.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799361ca",
   "metadata": {},
   "source": [
    "Open up the html file that was created in your web browser and explore the topics that were found.\n",
    "\n",
    "**Question:** How does the relevance metric change as the parameter lambda goes from 0 to 1?\n",
    "\n",
    "**Question:** Look at the topic labeled as topic 6 in the visualization. What do papers related to this topic seem to be about?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4876b137",
   "metadata": {},
   "source": [
    "Once our model is fit, we can get the topic distribution for each document. Take a look at the topic distribution for the document with id 100. Does this topic distribution look reasonable, given the visualization?\n",
    "\n",
    "**Warning:** The pyldavis library starts counting at 1, whereas the gensim library starts counting at 0, so topic 1 in the html document really corresponds to topic 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c9637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "\n",
    "print(f'Abstract: {papers.loc[i, \"abstract\"]}')\n",
    "model.get_document_topics(corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b732582",
   "metadata": {},
   "source": [
    "Now, build a DataFrame which has, for each document, the topic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca095181",
   "metadata": {},
   "outputs": [],
   "source": [
    "### REMOVE\n",
    "topic_dist = pd.DataFrame([{key: value for key,value in model[paper]} for paper in corpus],\n",
    "                         columns = list(range(8))).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d5883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4f28d1",
   "metadata": {},
   "source": [
    "Find a paper that has the highest makeup of topic 5. Then look at the abstract of this paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c2d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf38688",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dist.nlargest(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93073b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers.loc[1173, 'abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b1d4ab",
   "metadata": {},
   "source": [
    "**Challenge Question:** Pick two topics and find a paper which is made up of about 50% of each of those topics. Hint: You could use the cosine similarity to find such a paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4832160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd1efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(cosine_similarity(np.array([1, 0, 0, 0, 0, 1, 0, 0]).reshape(1, -1), topic_dist[list(range(8))]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1224d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dist.loc[1396]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f68f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers.loc[1234, 'abstract']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
