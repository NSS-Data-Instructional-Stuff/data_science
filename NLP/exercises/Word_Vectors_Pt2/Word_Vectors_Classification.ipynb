{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll see if we can do better in predicting sentiment by using word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Gnostic Gospels (Vintage)</td>\n",
       "      <td>This is a misrepesentation of the Gospels. It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Christine Feehan sucks</td>\n",
       "      <td>Ok she always starts off good with the tension...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>bad review</td>\n",
       "      <td>The Dvd that amazon sent me only worked one ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Cheap</td>\n",
       "      <td>This bracelet was missing pearls, and when I e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>piece of crap</td>\n",
       "      <td>The ear piece is completely worthless. It is c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                          title  \\\n",
       "0          1  The Gnostic Gospels (Vintage)   \n",
       "1          1         Christine Feehan sucks   \n",
       "2          1                     bad review   \n",
       "3          1                          Cheap   \n",
       "4          1                  piece of crap   \n",
       "\n",
       "                                                text  \n",
       "0  This is a misrepesentation of the Gospels. It ...  \n",
       "1  Ok she always starts off good with the tension...  \n",
       "2  The Dvd that amazon sent me only worked one ti...  \n",
       "3  This bracelet was missing pearls, and when I e...  \n",
       "4  The ear piece is completely worthless. It is c...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv('data/amazon_reviews.csv')\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reviews[['text']]\n",
    "y = reviews['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get our baselines using CountVectorizer and TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer()\n",
    "\n",
    "X_train_vec = vect.fit_transform(X_train['text'])\n",
    "X_test_vec = vect.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1003,  247],\n",
       "       [ 236, 1014]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter = 10000).fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test_vec)\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "\n",
    "X_train_vec = vect.fit_transform(X_train['text'])\n",
    "X_test_vec = vect.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1030,  220],\n",
       "       [ 224, 1026]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter = 10000).fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test_vec)\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using a matrix-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import svds\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_counter = Counter()\n",
    "\n",
    "for review in X_train['text']:\n",
    "    review_counter.update(gensim.utils.simple_preprocess(review))\n",
    "\n",
    "word_index = {word: i for i, word in enumerate(review_counter.keys())}\n",
    "index_word = {i: word for i, word in enumerate(review_counter.keys())}\n",
    "\n",
    "window_size = 2\n",
    "\n",
    "cooccurrence_counter = Counter()\n",
    "\n",
    "for review in X_train['text']:\n",
    "    # First, tokenize the sentence\n",
    "    sentence = gensim.utils.simple_preprocess(review)\n",
    "    \n",
    "    # Then, we'll build the window around each word\n",
    "    for i, word in enumerate(sentence):\n",
    "        window = sentence[max(0, i-2): i] + sentence[i+1: i+3]\n",
    "\n",
    "        # Then, we'll up the counter value for that pair\n",
    "        for other_word in window:\n",
    "            cooccurrence_counter[(word, other_word)] += 1\n",
    "            \n",
    "\n",
    "for word in review_counter.keys():\n",
    "    cooccurrence_counter[(word, word)] += review_counter[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_idx = []\n",
    "col_idx = []\n",
    "counts = []\n",
    "\n",
    "for (word1, word2) in cooccurrence_counter.keys():\n",
    "    row_idx.append(word_index[word1])\n",
    "    col_idx.append(word_index[word2])\n",
    "    counts.append(cooccurrence_counter[(word1, word2)])\n",
    "\n",
    "cooccurrence_matrix = sparse.csc_matrix((counts, (row_idx, col_idx)), dtype = 'float')\n",
    "\n",
    "dimension = 50\n",
    "\n",
    "U, D, V = svds(cooccurrence_matrix, k = dimension)\n",
    "\n",
    "word_vectors = U * D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix above gives us vectors for each individual work. But how do we vectorize a full review?\n",
    "\n",
    "A common approach is to take the average of all of the word vectors in that review. Let's write some code to accomplish this.\n",
    "\n",
    "We'll grab the first review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a misrepesentation of the Gospels. It should be noted that this is not a translation of the Gnostic Gospel. It is merely The authors viewpoints and excerps from the original text.Which, by the way, has been translated.I was under the impression that this was the tranlated Gnostic Bibles. This book should be clearly titled \"My thoughts of the Gnostic text\".Don\\'t bother ordering this book if you are looking for the Gnostic Bible translated.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = X_train['text'][0]\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create an initial vector of zeros. Do this using the numpy `zeros` function. Make sure that the result will have the same shape as the word vectors from the matrix.\n",
    "\n",
    "We'll also need to count the number of words in the review for which we have an embedding. We'll initialize a counter variable at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (240307330.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_179746/240307330.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    embedding = # Fill this in\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "embedding = # Fill this in\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write a for loop which will go through each word in the preprocessed text, check if that word appears in the matrix (using the `word_index` dictionary), and if so, add its embedding to the embedding and increment the counter by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1007975269.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_179746/1007975269.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    if count > 0:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "for word in gensim.utils.simple_preprocess(review):\n",
    "    #### Fill in the body of the for loop\n",
    "    \n",
    "if count > 0:\n",
    "    embedding /= count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to apply this to all of the reviews, convert your code above into a function which can take the text of a review and return an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (794166411.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_193346/794166411.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    # Fill this in\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def make_vector_matrix(review):\n",
    "    # Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to use this function to vectorize each review in the training and the test sets.\n",
    "\n",
    "Create two variables, `X_train_vec` and `X_test_vec` by applying your function to the text of the train and test reviews. Hint: you may need to use the [numpy `vstack` function](https://numpy.org/doc/stable/reference/generated/numpy.vstack.html) to convert the results to a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = # Fill this in\n",
    "X_test_vec = # Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how these vectors did compared to the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[831, 419],\n",
       "       [381, 869]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter = 10000).fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test_vec)\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try a different type of word vector - word2vec. This model is implemented in the [gensim library](https://radimrehurek.com/gensim/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Word2Vec model](https://radimrehurek.com/gensim/models/word2vec.html) expects a collection of tokenized sentences. We'll use gensim's simple_preprocess function to tokenize and clean up the text of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [gensim.utils.simple_preprocess(sentence) for sentence in X_train['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of hyperparameters we can set. You can feel free to change these or leave them to the current values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(train_sentences,\n",
    "                              vector_size = 100,\n",
    "                              window = 5,\n",
    "                              min_count = 2,\n",
    "                              sg = 1,\n",
    "                              hs = 0,\n",
    "                              negative = 10,\n",
    "                              epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is fit, you can access the word vectors through `.wv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6060952 ,  0.35809305, -0.20759344,  0.18713778, -0.19552566,\n",
       "       -0.46109164,  0.7681601 ,  0.65415114, -0.12236015, -0.44984972,\n",
       "        0.51400286, -0.05475419,  0.5449332 ,  0.17944384, -0.20722571,\n",
       "       -1.1557525 ,  0.55389464, -0.28227523, -0.15193258, -0.23581949,\n",
       "        0.18039699, -0.2276723 ,  0.5934985 , -0.5953563 ,  0.8057621 ,\n",
       "       -0.15314746, -0.65783834, -0.5568405 , -0.25253367,  0.26088655,\n",
       "       -0.16711307,  0.24099213,  0.2011326 ,  0.43268412, -0.8698468 ,\n",
       "        0.84646153, -0.03731446,  0.05006408,  0.26601744, -0.7598717 ,\n",
       "       -0.8653652 , -0.62019724,  0.3869248 ,  0.07545926, -0.00375254,\n",
       "        0.00327626, -0.54103774,  0.19690773, -0.09124593,  0.21769074,\n",
       "       -0.21526293, -0.9890786 , -0.5045264 ,  0.3277378 ,  0.00262126,\n",
       "       -0.64682233, -0.42446283,  0.26766402, -0.12486582, -0.01101785,\n",
       "       -0.8928909 , -0.07323002,  0.19150165, -0.2556738 ,  0.6177432 ,\n",
       "       -0.33525002, -0.2786363 ,  0.47334754,  0.54774565,  0.32791063,\n",
       "        0.47638106,  0.38597366, -0.00204623,  0.00716621, -0.83087575,\n",
       "        0.03065809, -0.5683392 ,  0.00850277,  0.47378826,  0.9070349 ,\n",
       "       -0.122563  ,  0.16565992,  0.22871336, -0.20618942,  0.10166495,\n",
       "        0.22452532,  0.34185666, -0.21939678,  0.7363504 , -0.32863593,\n",
       "        0.41251847,  0.3503404 ,  0.26585457,  0.56829894,  0.87435365,\n",
       "        0.9089459 ,  0.09026656, -0.5200995 ,  0.5993095 ,  0.17023493],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.get_vector('pc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also quickly find the most similar word for a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('compaq', 0.6008726358413696),\n",
       " ('demos', 0.5389522314071655),\n",
       " ('celeron', 0.5211091041564941),\n",
       " ('windows', 0.4956669211387634),\n",
       " ('nvidia', 0.49237313866615295),\n",
       " ('iphone', 0.49212580919265747),\n",
       " ('overkill', 0.48985496163368225),\n",
       " ('patched', 0.48859703540802),\n",
       " ('system', 0.48829033970832825),\n",
       " ('ati', 0.48686859011650085)]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('pc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to find the average embedding for the text of a review. \n",
    "Then use this function to vectorize the training and test reviews. How do the word2vec vectors compare to the other approaches on this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vector(review):\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = # Fill this in\n",
    "X_test_vec = # Fill this in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[960, 290],\n",
       "       [261, 989]])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter = 10000).fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test_vec)\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, gensim has a number of pretrained word vector models. Let's try out the Glove Wikipedia Gigaword 100 model, which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('desktop', 0.7962974905967712),\n",
       " ('pcs', 0.765652060508728),\n",
       " ('macintosh', 0.7592843174934387),\n",
       " ('computer', 0.7366448640823364),\n",
       " ('computers', 0.7268315553665161),\n",
       " ('hardware', 0.7222781777381897),\n",
       " ('playstation', 0.7164796590805054),\n",
       " ('software', 0.7147746682167053),\n",
       " ('console', 0.7037823796272278),\n",
       " ('xbox', 0.6895698308944702)]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('pc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to make a vector for each review using these embeddings. How well does the model do using these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vector(review):\n",
    "# fill this in\n",
    "\n",
    "X_train_vec = # fill this in\n",
    "X_test_vec = # fill this in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[949, 301],\n",
       "       [327, 923]])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter = 10000).fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test_vec)\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
