{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be working with a set of reviews from Amazon.com. This is a subset of a larger dataset obtained from https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Gnostic Gospels (Vintage)</td>\n",
       "      <td>This is a misrepesentation of the Gospels. It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Christine Feehan sucks</td>\n",
       "      <td>Ok she always starts off good with the tension...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>bad review</td>\n",
       "      <td>The Dvd that amazon sent me only worked one ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Cheap</td>\n",
       "      <td>This bracelet was missing pearls, and when I e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>piece of crap</td>\n",
       "      <td>The ear piece is completely worthless. It is c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                          title  \\\n",
       "0          1  The Gnostic Gospels (Vintage)   \n",
       "1          1         Christine Feehan sucks   \n",
       "2          1                     bad review   \n",
       "3          1                          Cheap   \n",
       "4          1                  piece of crap   \n",
       "\n",
       "                                                text  \n",
       "0  This is a misrepesentation of the Gospels. It ...  \n",
       "1  Ok she always starts off good with the tension...  \n",
       "2  The Dvd that amazon sent me only worked one ti...  \n",
       "3  This bracelet was missing pearls, and when I e...  \n",
       "4  The ear piece is completely worthless. It is c...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv('data/amazon_reviews.csv')\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each review is assigned a sentiment score, where 1 indicates negative and 2 indicates positive.\n",
    "\n",
    "This subset is equally balanced between positive and negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5000\n",
       "2    5000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at some of the negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a trim carpenter I have had three LS1212 saws all three saws have had the same problems. the bevel and miter adjustments start to stick and bind after about two months of use. Makita service is terrible. They have lied to me send saw back not fully adjusted. It took them four weeks to do what should have been a simple fix. It is a nice when it works right but you need a back up saw for when it breaks. Makita has disapointed me. I will never buy another Makita tool.Their products and service are first rate GARBAGE.\n",
      "-----------------------------\n",
      "The heater worked great in my 3 gallon eclipse for about 3 days or so. It kept my tank right at 78 degrees which is what I set it to. However, after that bad things started to happen. First, the temperature got to be 80, then 82 and then even when I set to heater to 76F, my tank was 85F and the heater was still not shutting off. I thought it was my thermometer, but it wasn't. I'm just glad it didn't get any hotter or my poor betta would have not been in a good shape. I'm sending mine back for a replacement in the hopes that I just got a bad one. Still, it's a pain to ship this stuff back.\n",
      "-----------------------------\n",
      "This router will take a legitimate connection to a web site and turn it into an ad for belkin products! The older models don't but if you update the firmware (like maybe to close a security hole in the future, or for compatibility of a network game) you will start getting ads! There are a couple of articles that are at popular news organizations that confirm this. BOYCOTT, DO NOT UPGRADE THE FIRMWARE!\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "seed = 123\n",
    "for statement in reviews.loc[reviews['sentiment'] == 1, 'text'].sample(3, random_state=seed):\n",
    "    print(statement)\n",
    "    print('-----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then some of the positive ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Much like it's predecessor Darwinia, Bios is an ambitious combination of hard science fiction and pan-universal spirituality. Wilson writes beautifully; I'm envious. Of particular interest is his treatment of emerging biohazards. Let's hope this book isn't as prophetic as it may seem to be. If you really appreciate literature and science fiction (and especially a combo of the two) you owe it to yourself to read both Bios and Darwinia.\n",
      "-----------------------------\n",
      "this epesode told us bout how mac gyver never give up, even the person who he help hate him & insult him at the first he tried to help her.\n",
      "-----------------------------\n",
      "I've gotten to know Tatiana and her producer/manager/husband Matthew, over the last few years. In a live concert, from when she plays that first chord on her keyboard and opens her mouth -- shivers run up the spine of everyone in the audience. She's an amazing talent who can hit a perfect pitch with all the power and inspiriation of one of the clearest voices alive today. She's been labled as the Streisand of Catholicism. What is amazing about this DVD (which comes in Dolby 5.1 Surround) is that the power, clarity and talent of this young woman is translated right into your living room...as long as you have a decent audio system. Crank up the volume, it's worth it, thanks to the production expertise of Jack Gorton and audio desinger Tom Weir...oh, and Tatiana.\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "seed = 123\n",
    "for statement in reviews.loc[reviews['sentiment'] == 2, 'text'].sample(3, random_state=seed):\n",
    "    print(statement)\n",
    "    print('-----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Naive Bayes Using the Text Field\n",
    "\n",
    "First, we'll split out data into a train and test set, stratifying on the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reviews[['text']]\n",
    "y = reviews['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way to convert the text into predictors. We will start by using a **bag of words** model - one which looks at what words are present but disregarding word order.\n",
    "\n",
    "Let's start with a [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "Fill in the code to import the CountVectorizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######REMOVE\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this in to import the CountVectorizer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a CountVectorizer (with all of the default arguments) named `vect` and fit it to the \"text\" column of the training data transform both the train and test texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######REMOVE\n",
    "vect = CountVectorizer()\n",
    "\n",
    "X_train_vec = vect.fit_transform(X_train['text'])\n",
    "X_test_vec = vect.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (869380711.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_345287/869380711.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    vect = # Fill this in\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Fill in the code to fit and transform a CountVectorizer (using all defaults) on the text column of X_train and X_test\n",
    "\n",
    "vect = # Fill this in\n",
    "\n",
    "X_train_vec = # Fill this in\n",
    "X_test_vec = # Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CountVectorizer class will take in the text, and **tokenize** it, splitting it into a list of tokens. The built-in tokenizer does some minimal cleaning of the text, and by default the CountVectorizer will convert all text to lowercase.\n",
    "\n",
    "If we want to take a look at all of the tokens that the CountVectorizer has seen, we can look at its vocabulary. Check the `vocabulary_` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill this in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 25040,\n",
       " 'was': 27139,\n",
       " 'probally': 19291,\n",
       " 'the': 24914,\n",
       " '2nd': 337,\n",
       " 'best': 2873,\n",
       " 'movie': 16380,\n",
       " 'ive': 13308,\n",
       " 'seen': 21971,\n",
       " 'acting': 868,\n",
       " 'great': 11048,\n",
       " 'and': 1508,\n",
       " 'josh': 13570,\n",
       " 'jackson': 13333,\n",
       " 'hotter': 12144,\n",
       " 'than': 24888,\n",
       " 'ever': 8845,\n",
       " 'is': 13247,\n",
       " 'must': 16523,\n",
       " 'see': 21954,\n",
       " 'it': 13280,\n",
       " 'about': 702,\n",
       " 'time': 25194,\n",
       " 'what': 27346,\n",
       " 'taking': 24524,\n",
       " 'so': 22976,\n",
       " 'long': 14807,\n",
       " 'for': 9970,\n",
       " 'studios': 23965,\n",
       " 'to': 25263,\n",
       " 'release': 20492,\n",
       " 'these': 24988,\n",
       " 'rarer': 19972,\n",
       " 'movies': 16382,\n",
       " 'also': 1349,\n",
       " 'recently': 20153,\n",
       " 'released': 20493,\n",
       " 'butcher': 3862,\n",
       " 'boy': 3418,\n",
       " 'on': 17389,\n",
       " 'dvd': 7993,\n",
       " 'from': 10252,\n",
       " 'one': 17394,\n",
       " 'format': 10031,\n",
       " 'vhs': 26747,\n",
       " 'hd': 11576,\n",
       " 'now': 17095,\n",
       " 'coming': 5222,\n",
       " 'into': 13095,\n",
       " 'own': 17782,\n",
       " 'films': 9613,\n",
       " 'have': 11542,\n",
       " 'passed': 18034,\n",
       " 'initial': 12811,\n",
       " 'decade': 6582,\n",
       " 'of': 17294,\n",
       " 'that': 24899,\n",
       " 'ridiculous': 21009,\n",
       " 'not': 17043,\n",
       " '1930': 168,\n",
       " 'print': 19258,\n",
       " 'needs': 16757,\n",
       " 'years': 27920,\n",
       " 'cleaning': 4891,\n",
       " 'get': 10646,\n",
       " 'them': 24937,\n",
       " 'out': 17618,\n",
       " 'faster': 9372,\n",
       " 'really': 20084,\n",
       " 'wanting': 27079,\n",
       " 'read': 20048,\n",
       " 'christmas': 4731,\n",
       " 'story': 23811,\n",
       " 'aside': 1991,\n",
       " 'fact': 9225,\n",
       " 'there': 24968,\n",
       " 'some': 23076,\n",
       " 'snow': 22959,\n",
       " 'ground': 11147,\n",
       " 'wonderful': 27637,\n",
       " 'house': 12155,\n",
       " 'in': 12557,\n",
       " 'connecticut': 5539,\n",
       " 'wasn': 27150,\n",
       " 'much': 16418,\n",
       " 'holiday': 11989,\n",
       " 'an': 1472,\n",
       " 'ok': 17345,\n",
       " 'however': 12172,\n",
       " 'especially': 8744,\n",
       " 'if': 12372,\n",
       " 'you': 27965,\n",
       " 'believe': 2780,\n",
       " 'reincarnation': 20445,\n",
       " 'my': 16551,\n",
       " 'favorites': 9408,\n",
       " 'but': 3860,\n",
       " 'would': 27735,\n",
       " 'venture': 26685,\n",
       " 'another': 1621,\n",
       " 'graham': 10972,\n",
       " 'novel': 17084,\n",
       " 'came': 4008,\n",
       " 'highly': 11855,\n",
       " 'recommended': 20213,\n",
       " 'guy': 11260,\n",
       " 'de': 6534,\n",
       " 'rosalos': 21256,\n",
       " 'angeles': 1542,\n",
       " 'california': 3977,\n",
       " 'be': 2620,\n",
       " 'careful': 4151,\n",
       " 'when': 27359,\n",
       " 'buying': 3882,\n",
       " 'product': 19326,\n",
       " 'most': 16310,\n",
       " 'snes': 22936,\n",
       " 'need': 16747,\n",
       " 'dc': 6526,\n",
       " '10v': 53,\n",
       " 'adapter': 922,\n",
       " 'bought': 3375,\n",
       " 'week': 27266,\n",
       " 'ago': 1133,\n",
       " 'doesnt': 7579,\n",
       " 'work': 27680,\n",
       " 'because': 2677,\n",
       " 'only': 17410,\n",
       " '9v': 632,\n",
       " 'book': 3270,\n",
       " 'misleading': 16044,\n",
       " 'title': 25245,\n",
       " 'content': 5656,\n",
       " 'think': 25024,\n",
       " 'terms': 24835,\n",
       " 'introduced': 13121,\n",
       " 'little': 14710,\n",
       " 'known': 13976,\n",
       " 'except': 8920,\n",
       " 'int': 12969,\n",
       " 'hacker': 11294,\n",
       " 'world': 27702,\n",
       " 'no': 16961,\n",
       " 'were': 27318,\n",
       " 'copy': 5796,\n",
       " 'pasted': 18048,\n",
       " 'third': 25033,\n",
       " 'party': 18024,\n",
       " 'source': 23189,\n",
       " 'care': 4145,\n",
       " 'mention': 15715,\n",
       " 'don': 7616,\n",
       " 'waste': 27155,\n",
       " 'your': 27976,\n",
       " 'money': 16199,\n",
       " 'such': 24092,\n",
       " 'worthless': 27726,\n",
       " 'trash': 25598,\n",
       " 'two': 25916,\n",
       " 'people': 18229,\n",
       " 'who': 27412,\n",
       " 'gave': 10524,\n",
       " 'crap': 6024,\n",
       " 'star': 23588,\n",
       " 'review': 20911,\n",
       " 'are': 1852,\n",
       " 'probably': 19290,\n",
       " 'affiliated': 1063,\n",
       " 'with': 27577,\n",
       " 'author': 2219,\n",
       " 'want': 27075,\n",
       " 'high': 11843,\n",
       " 'quality': 19733,\n",
       " 'flashlight': 9768,\n",
       " 'red': 20268,\n",
       " 'led': 14379,\n",
       " 'quite': 19810,\n",
       " 'dim': 7195,\n",
       " 'by': 3892,\n",
       " 'experience': 9049,\n",
       " 'other': 17600,\n",
       " 'flashlights': 9769,\n",
       " 'ranks': 19946,\n",
       " 'at': 2087,\n",
       " 'bottom': 3369,\n",
       " 'list': 14679,\n",
       " 'granted': 11012,\n",
       " 'different': 7150,\n",
       " 'yet': 27933,\n",
       " 'like': 14586,\n",
       " 'white': 27404,\n",
       " 'blue': 3160,\n",
       " 'ones': 17399,\n",
       " 'ten': 24795,\n",
       " 'times': 25204,\n",
       " 'over': 17691,\n",
       " 'do': 7541,\n",
       " 'recommend': 20209,\n",
       " 'purchase': 19645,\n",
       " 'unless': 26260,\n",
       " 'change': 4461,\n",
       " 'color': 5151,\n",
       " 'bulb': 3758,\n",
       " 'which': 27373,\n",
       " 'know': 13968,\n",
       " 'possible': 18962,\n",
       " 'alot': 1341,\n",
       " 'unicorns': 26204,\n",
       " 'enter': 8572,\n",
       " 'beauty': 2671,\n",
       " 'pageant': 17855,\n",
       " 'flower': 9867,\n",
       " 'child': 4632,\n",
       " 'mandy': 15225,\n",
       " 'miller': 15919,\n",
       " 'against': 1102,\n",
       " 'as': 1966,\n",
       " 'terrible': 24839,\n",
       " 'thing': 25020,\n",
       " 'pretty': 19199,\n",
       " 'girl': 10720,\n",
       " 'miss': 16056,\n",
       " 'teen': 24736,\n",
       " 'sweet': 24364,\n",
       " 'valley': 26573,\n",
       " 'will': 27480,\n",
       " 'enjoy': 8531,\n",
       " 'unicorn': 26203,\n",
       " 'club': 5004,\n",
       " 'message': 15762,\n",
       " 'utmost': 26514,\n",
       " 'importance': 12514,\n",
       " 'told': 25305,\n",
       " 'through': 25104,\n",
       " 'amusing': 1466,\n",
       " 'stories': 23804,\n",
       " 'adventure': 1018,\n",
       " 'wilderness': 27472,\n",
       " 'we': 27217,\n",
       " 'carefully': 4152,\n",
       " 'preserve': 19167,\n",
       " 'trouts': 25761,\n",
       " 'extended': 9126,\n",
       " 'habitat': 11284,\n",
       " 'can': 4034,\n",
       " 'continue': 5677,\n",
       " 'pursue': 19675,\n",
       " 'wild': 27467,\n",
       " 'trout': 25760,\n",
       " 'take': 24517,\n",
       " 'or': 17498,\n",
       " 'maybe': 15504,\n",
       " 'fish': 9694,\n",
       " 'flavor': 9779,\n",
       " 'leave': 14366,\n",
       " 'rest': 20807,\n",
       " 'behind': 2753,\n",
       " 'where': 27364,\n",
       " 'they': 25000,\n",
       " 'raised': 19896,\n",
       " 'before': 2721,\n",
       " 'return': 20873,\n",
       " 'home': 12012,\n",
       " 'make': 15164,\n",
       " 'sure': 24255,\n",
       " 'everything': 8859,\n",
       " 'around': 1909,\n",
       " 'first': 9688,\n",
       " 'got': 10916,\n",
       " 'fans': 9322,\n",
       " 'doesn': 7577,\n",
       " 'even': 8834,\n",
       " 'come': 5198,\n",
       " 'close': 4976,\n",
       " 'holding': 11984,\n",
       " 'absolute': 716,\n",
       " 'mess': 15761,\n",
       " 'compared': 5301,\n",
       " 'dune': 7947,\n",
       " 'find': 9640,\n",
       " 'film': 9607,\n",
       " 'very': 26730,\n",
       " 'disapointing': 7261,\n",
       " 'nothing': 17058,\n",
       " 'whoa': 27413,\n",
       " 'just': 13667,\n",
       " 'watching': 27168,\n",
       " 'themselves': 24947,\n",
       " 'asking': 1997,\n",
       " 'every': 8852,\n",
       " 'minute': 16000,\n",
       " 'alien': 1264,\n",
       " 'had': 11300,\n",
       " 'better': 2892,\n",
       " 'effects': 8197,\n",
       " 'made': 15078,\n",
       " 'five': 9717,\n",
       " 'earlier': 8066,\n",
       " 'shields': 22315,\n",
       " 'look': 14820,\n",
       " 'horrible': 12110,\n",
       " 'ships': 22340,\n",
       " 'combat': 5182,\n",
       " 'looks': 14826,\n",
       " 'voices': 26935,\n",
       " 'laughable': 14251,\n",
       " 'couldn': 5905,\n",
       " 'give': 10728,\n",
       " 'though': 25070,\n",
       " 'cause': 4311,\n",
       " 'big': 2932,\n",
       " 'fan': 9306,\n",
       " 'decision': 6606,\n",
       " 'makers': 15168,\n",
       " 'digital': 7174,\n",
       " 'economy': 8139,\n",
       " 'aldrich': 1230,\n",
       " 'provides': 19500,\n",
       " 'reader': 20051,\n",
       " 'clear': 4898,\n",
       " 'vision': 26878,\n",
       " 'landscape': 14171,\n",
       " 'he': 11585,\n",
       " 'labels': 14082,\n",
       " 'language': 14182,\n",
       " 'descriptions': 6917,\n",
       " 'bring': 3592,\n",
       " 'context': 5665,\n",
       " 'finally': 9629,\n",
       " 'strategies': 23855,\n",
       " 'both': 3358,\n",
       " 'existing': 9004,\n",
       " 'start': 23610,\n",
       " 'up': 26411,\n",
       " 'companies': 5292,\n",
       " 'exploit': 9088,\n",
       " 'age': 1105,\n",
       " 'predictive': 19100,\n",
       " 'value': 26579,\n",
       " 'awesome': 2297,\n",
       " 'tells': 24765,\n",
       " 'signs': 22526,\n",
       " 'indicate': 12665,\n",
       " 'she': 22259,\n",
       " 'jerk': 13452,\n",
       " 'interesting': 13024,\n",
       " 'never': 16834,\n",
       " 'fall': 9273,\n",
       " 'love': 14892,\n",
       " 'after': 1086,\n",
       " 'reading': 20055,\n",
       " 'used': 26480,\n",
       " 'similar': 22559,\n",
       " 'style': 24004,\n",
       " 'folding': 9928,\n",
       " 'box': 3409,\n",
       " 'cutting': 6358,\n",
       " 'knife': 13944,\n",
       " 'ease': 8091,\n",
       " 'changing': 4465,\n",
       " 'blades': 3050,\n",
       " 'well': 27304,\n",
       " 'functions': 10324,\n",
       " 'too': 25348,\n",
       " 'feels': 9470,\n",
       " 'its': 13300,\n",
       " 'real': 20067,\n",
       " 'seems': 21970,\n",
       " 'cost': 5883,\n",
       " 'somewhere': 23095,\n",
       " '60': 507,\n",
       " 'dollars': 7597,\n",
       " 'male': 15181,\n",
       " 'strippers': 23915,\n",
       " 'elvin': 8340,\n",
       " 'bibiloni': 2918,\n",
       " 'gets': 10649,\n",
       " 'evicted': 8863,\n",
       " 'his': 11913,\n",
       " 'apartment': 1697,\n",
       " 'fabian': 9197,\n",
       " 'few': 9534,\n",
       " 'guys': 11261,\n",
       " 'help': 11729,\n",
       " 'him': 11884,\n",
       " 'move': 16373,\n",
       " 'encounter': 8453,\n",
       " 'problems': 19301,\n",
       " 'flashbacks': 9763,\n",
       " 'comes': 5208,\n",
       " 'drug': 7868,\n",
       " 'liquid': 14673,\n",
       " 'form': 10028,\n",
       " 'ecstasy': 8142,\n",
       " 'insulting': 12964,\n",
       " 'juvenile': 13680,\n",
       " 'humor': 12237,\n",
       " 'coarse': 5033,\n",
       " 'immature': 12458,\n",
       " 'opens': 17442,\n",
       " 'disclaimer': 7298,\n",
       " 'warning': 27112,\n",
       " 'race': 19836,\n",
       " 'religion': 20513,\n",
       " 'women': 27631,\n",
       " 'typically': 25933,\n",
       " 'called': 3985,\n",
       " 'word': 27670,\n",
       " 'compares': 5303,\n",
       " 'itself': 13301,\n",
       " 'south': 23192,\n",
       " 'park': 17985,\n",
       " 'lacks': 14108,\n",
       " 'cleverness': 4918,\n",
       " 'enough': 8552,\n",
       " 'howard': 12168,\n",
       " 'stern': 23704,\n",
       " 'type': 25929,\n",
       " 'adolescence': 988,\n",
       " 'then': 24948,\n",
       " 'certainly': 4408,\n",
       " 'hip': 11899,\n",
       " 'hop': 12078,\n",
       " 'music': 16506,\n",
       " 'parental': 17978,\n",
       " 'guide': 11208,\n",
       " 'bomb': 3241,\n",
       " 'heck': 11682,\n",
       " 'whole': 27415,\n",
       " 'alphabet': 1345,\n",
       " 'sex': 22153,\n",
       " 'nudity': 17123,\n",
       " 'listening': 14687,\n",
       " 'songs': 23108,\n",
       " 'preview': 19209,\n",
       " 'could': 5903,\n",
       " 'dissent': 7437,\n",
       " 'song': 23103,\n",
       " 'webber': 27249,\n",
       " 'musicals': 16511,\n",
       " 'phantom': 18385,\n",
       " 'opera': 17443,\n",
       " 'lot': 14869,\n",
       " 'tunes': 25836,\n",
       " 'puccini': 19578,\n",
       " 'rimsky': 21047,\n",
       " 'korsakov': 14003,\n",
       " 'rearranged': 20100,\n",
       " 'beautifully': 2670,\n",
       " 'should': 22406,\n",
       " 'done': 7621,\n",
       " 'same': 21520,\n",
       " 'woman': 27629,\n",
       " 'melodic': 15666,\n",
       " 'rearrange': 20099,\n",
       " 'least': 14360,\n",
       " 'ask': 1995,\n",
       " 'burgh': 3813,\n",
       " 'permission': 18297,\n",
       " 'use': 26477,\n",
       " 'melody': 15671,\n",
       " 'please': 18700,\n",
       " 'compare': 5300,\n",
       " 'how': 12167,\n",
       " 'many': 15290,\n",
       " 'left': 14387,\n",
       " 'reviews': 20918,\n",
       " 'selling': 22003,\n",
       " 'cd': 4337,\n",
       " 'eyes': 9177,\n",
       " 'beholder': 2756,\n",
       " 'limit': 14615,\n",
       " 'her': 11761,\n",
       " 'hauntingly': 11539,\n",
       " 'beautiful': 2668,\n",
       " 'alto': 1371,\n",
       " 'voice': 26932,\n",
       " 'driving': 7838,\n",
       " 'beat': 2649,\n",
       " 'melodies': 15668,\n",
       " 'kim': 13880,\n",
       " 'hill': 11874,\n",
       " 'has': 11502,\n",
       " 'winner': 27535,\n",
       " 'here': 11772,\n",
       " 'singer': 22600,\n",
       " 'more': 16273,\n",
       " 'collaboration': 5112,\n",
       " 'wayne': 27205,\n",
       " 'kirkpatrick': 13914,\n",
       " 'writes': 27787,\n",
       " 'thought': 25073,\n",
       " 'provoking': 19510,\n",
       " 'lyrics': 15026,\n",
       " 'personal': 18324,\n",
       " 'include': 12599,\n",
       " 'satisfied': 21614,\n",
       " 'all': 1281,\n",
       " 'non': 16988,\n",
       " 'christian': 4723,\n",
       " 'friends': 10225,\n",
       " 'mysterious': 16567,\n",
       " 'ways': 27206,\n",
       " 'wait': 27012,\n",
       " 'lord': 14849,\n",
       " 'worth': 27724,\n",
       " 'price': 19219,\n",
       " 'album': 1216,\n",
       " 'worst': 27722,\n",
       " 'books': 3282,\n",
       " 've': 26636,\n",
       " 'total': 25415,\n",
       " 'paper': 17941,\n",
       " 'printed': 19260,\n",
       " 'rating': 19990,\n",
       " 'system': 24465,\n",
       " 'wouldn': 27738,\n",
       " 'let': 14470,\n",
       " 'me': 15571,\n",
       " 'stars': 23607,\n",
       " 'early': 8069,\n",
       " 'patricia': 18081,\n",
       " 'cornwell': 5835,\n",
       " 'extremely': 9158,\n",
       " 'disappointed': 7270,\n",
       " 'characters': 4495,\n",
       " 'flat': 9772,\n",
       " 'plot': 18724,\n",
       " 'boring': 3333,\n",
       " 'writing': 27788,\n",
       " 'definitely': 6698,\n",
       " 'usual': 26498,\n",
       " 'standards': 23566,\n",
       " 'save': 21637,\n",
       " 'buy': 3877,\n",
       " 'something': 23088,\n",
       " 'official': 17316,\n",
       " 'rehashes': 20432,\n",
       " 'information': 12775,\n",
       " 'included': 12600,\n",
       " 'game': 10435,\n",
       " 'manual': 15274,\n",
       " 'does': 7576,\n",
       " 'mechanics': 15607,\n",
       " 'scores': 21785,\n",
       " 'calculated': 3957,\n",
       " 'individual': 12684,\n",
       " 'scenarios': 21710,\n",
       " 'winning': 27538,\n",
       " 'applied': 1764,\n",
       " 'cases': 4237,\n",
       " 'download': 7714,\n",
       " 'good': 10866,\n",
       " 'unofficial': 26295,\n",
       " 'tropico': 25746,\n",
       " 'strategy': 23856,\n",
       " 'gamespot': 10445,\n",
       " 'com': 5178,\n",
       " 'sucked': 24096,\n",
       " 'honestly': 12046,\n",
       " 'put': 19689,\n",
       " 'down': 7704,\n",
       " 'cast': 4255,\n",
       " 'feel': 9465,\n",
       " 'their': 24933,\n",
       " 'heads': 11607,\n",
       " '100': 22,\n",
       " 'amazing': 1399,\n",
       " 'common': 5263,\n",
       " 'practice': 19042,\n",
       " 'break': 3508,\n",
       " 'reviewers': 20914,\n",
       " 'reviewed': 20912,\n",
       " 'looking': 14824,\n",
       " 'person': 18320,\n",
       " 'advertise': 1024,\n",
       " 'website': 27255,\n",
       " 'say': 21659,\n",
       " 'okay': 17347,\n",
       " 'complete': 5351,\n",
       " 'still': 23738,\n",
       " 'question': 19763,\n",
       " 'end': 8466,\n",
       " 'why': 27427,\n",
       " 'did': 7117,\n",
       " 'jesuit': 13468,\n",
       " 'priests': 19233,\n",
       " 'fail': 9244,\n",
       " 'inquire': 12859,\n",
       " 'spiritual': 23349,\n",
       " 'life': 14551,\n",
       " 'aliens': 1269,\n",
       " 'main': 15146,\n",
       " 'character': 4485,\n",
       " 'continued': 5678,\n",
       " 'spirituality': 23350,\n",
       " 'existed': 8999,\n",
       " 'strength': 23886,\n",
       " 'course': 5944,\n",
       " 'inquired': 12860,\n",
       " 'found': 10083,\n",
       " 'devoid': 7044,\n",
       " 'might': 15882,\n",
       " 'flags': 9738,\n",
       " 'ruined': 21360,\n",
       " 'tale': 24529,\n",
       " 'contrived': 5707,\n",
       " 'blood': 3139,\n",
       " 'pressure': 19183,\n",
       " 'been': 2709,\n",
       " 'slowly': 22830,\n",
       " 'climing': 4947,\n",
       " 'past': 18045,\n",
       " 'went': 27312,\n",
       " 'saw': 21652,\n",
       " 'doctor': 7552,\n",
       " 'name': 16611,\n",
       " 'differant': 7145,\n",
       " 'monitor': 16208,\n",
       " 'manufactures': 15285,\n",
       " 'omron': 17388,\n",
       " 'hem': 11740,\n",
       " '780': 563,\n",
       " 'line': 14630,\n",
       " 'suggested': 24128,\n",
       " 'didn': 7122,\n",
       " 'monitering': 16207,\n",
       " 'wifes': 27451,\n",
       " 'persons': 18334,\n",
       " 'easy': 8103,\n",
       " 'measure': 15591,\n",
       " 'morning': 16282,\n",
       " 'evening': 8836,\n",
       " 'doctors': 7555,\n",
       " 'office': 17313,\n",
       " 'go': 10817,\n",
       " 'results': 20833,\n",
       " 'anyone': 1677,\n",
       " 'needing': 16749,\n",
       " 'accurate': 810,\n",
       " 'again': 1100,\n",
       " 'phone': 18426,\n",
       " 'daughter': 6501,\n",
       " 'loves': 14904,\n",
       " 'play': 18673,\n",
       " 'cell': 4367,\n",
       " 'learning': 14353,\n",
       " 'activities': 886,\n",
       " 'volume': 26948,\n",
       " 'room': 21242,\n",
       " 'completely': 5353,\n",
       " 'silent': 22531,\n",
       " 'order': 17518,\n",
       " 'hear': 11629,\n",
       " 'weeks': 27271,\n",
       " 'later': 14229,\n",
       " 'stopped': 23792,\n",
       " 'working': 27691,\n",
       " 'together': 25294,\n",
       " 'problem': 19296,\n",
       " 'shadows': 22178,\n",
       " 'sunlight': 24184,\n",
       " 'shinning': 22329,\n",
       " 'thru': 25114,\n",
       " 'trees': 25644,\n",
       " 'off': 17299,\n",
       " 'thats': 24907,\n",
       " 'going': 10844,\n",
       " 'effect': 8193,\n",
       " 'battery': 2588,\n",
       " 'hope': 12079,\n",
       " 'keep': 13758,\n",
       " 'critters': 6153,\n",
       " 'garden': 10472,\n",
       " 'guitar': 11227,\n",
       " 'may': 15500,\n",
       " 'internet': 13053,\n",
       " 'sucks': 24102,\n",
       " 'wrong': 27793,\n",
       " '3rd': 398,\n",
       " 'string': 23907,\n",
       " 'spare': 23231,\n",
       " 'right': 21029,\n",
       " 'sells': 22005,\n",
       " 'guitars': 11230,\n",
       " 'makes': 15169,\n",
       " 'open': 17431,\n",
       " 'package': 17826,\n",
       " 'smells': 22870,\n",
       " 'spraypaint': 23424,\n",
       " 'chip': 4661,\n",
       " 'black': 3035,\n",
       " 'paint': 17870,\n",
       " 'frets': 10209,\n",
       " 'pick': 18472,\n",
       " 'cheap': 4545,\n",
       " 'opinion': 17456,\n",
       " 'suggest': 24127,\n",
       " 'nobody': 16966,\n",
       " 'buys': 3883,\n",
       " 'waited': 27014,\n",
       " 'become': 2684,\n",
       " 'available': 2255,\n",
       " 'cinema': 4790,\n",
       " 'almost': 1331,\n",
       " '20': 248,\n",
       " 'enjoyable': 8532,\n",
       " 'stellar': 23681,\n",
       " 'classic': 4864,\n",
       " 'performances': 18265,\n",
       " 'combine': 5188,\n",
       " 'gilbert': 10700,\n",
       " 'sullivan': 24151,\n",
       " 'faturing': 9391,\n",
       " 'famous': 9305,\n",
       " 'policeman': 18816,\n",
       " 'happy': 11434,\n",
       " 'riot': 21068,\n",
       " 'finish': 9659,\n",
       " 'having': 11548,\n",
       " 'returned': 20875,\n",
       " 'africa': 1080,\n",
       " 'hungry': 12253,\n",
       " 'facinating': 9222,\n",
       " 'continent': 5670,\n",
       " 'kingsolver': 13900,\n",
       " 'favorite': 9407,\n",
       " 'writer': 27785,\n",
       " 'delighted': 6761,\n",
       " 'savored': 21648,\n",
       " 'length': 14434,\n",
       " 'detail': 6985,\n",
       " 'else': 8329,\n",
       " 'fear': 9430,\n",
       " 'satisfy': 21616,\n",
       " 'deal': 6541,\n",
       " 'finished': 9660,\n",
       " 'handmaid': 11390,\n",
       " 'troubling': 25757,\n",
       " 'delicious': 6758,\n",
       " 'talk': 24537,\n",
       " 'everyone': 8856,\n",
       " 'evolution': 8879,\n",
       " 'history': 11930,\n",
       " 'learned': 14350,\n",
       " 'am': 1385,\n",
       " 'wondering': 27639,\n",
       " 'school': 21745,\n",
       " 'literature': 14701,\n",
       " 'classes': 4863,\n",
       " 'areas': 1854,\n",
       " 'hot': 12139,\n",
       " 'switch': 24395,\n",
       " 'bad': 2381,\n",
       " 'location': 14765,\n",
       " 'trying': 25795,\n",
       " 'roll': 21202,\n",
       " 'hair': 11319,\n",
       " 'brush': 3673,\n",
       " 'inadvertently': 12567,\n",
       " 'shut': 22465,\n",
       " 'takes': 24522,\n",
       " 'forever': 10005,\n",
       " 'heat': 11655,\n",
       " 'curl': 6302,\n",
       " 'safe': 21463,\n",
       " 'area': 1853,\n",
       " 'hold': 11980,\n",
       " 'away': 2292,\n",
       " 'curling': 6305,\n",
       " 'barrel': 2534,\n",
       " 'burn': 3819,\n",
       " 'fingers': 9655,\n",
       " 'planning': 18645,\n",
       " 'item': 13291,\n",
       " 'soon': 23126,\n",
       " 'replacement': 20650,\n",
       " 'any': 1672,\n",
       " 'suggestions': 24131,\n",
       " 'purchased': 19646,\n",
       " 'several': 22142,\n",
       " 'ourselves': 17617,\n",
       " 'others': 17601,\n",
       " 'gifts': 10687,\n",
       " 'presentation': 19161,\n",
       " 'peaceful': 18141,\n",
       " 'literate': 14700,\n",
       " 'progressive': 19373,\n",
       " 'society': 23002,\n",
       " 'destroyed': 6975,\n",
       " 'self': 21994,\n",
       " 'serving': 22116,\n",
       " 'american': 1417,\n",
       " 'citizens': 4821,\n",
       " 'using': 26493,\n",
       " 'military': 15904,\n",
       " 'forces': 9981,\n",
       " 'united': 26237,\n",
       " 'states': 23631,\n",
       " 'overthrow': 17759,\n",
       " 'violent': 26847,\n",
       " 'discrimanatory': 7333,\n",
       " 'nation': 16668,\n",
       " 'shame': 22207,\n",
       " 'forgotten': 10024,\n",
       " 'resulted': 20831,\n",
       " 'strong': 23925,\n",
       " 'undercurrent': 26086,\n",
       " 'prejudice': 19122,\n",
       " 'wish': 27566,\n",
       " 'major': 15158,\n",
       " 'throughout': 25106,\n",
       " 'evil': 8872,\n",
       " 'spring': 23431,\n",
       " 'missionaries': 16062,\n",
       " 'became': 2674,\n",
       " 'playstation': 18692,\n",
       " 'games': 10442,\n",
       " 'clone': 4971,\n",
       " 'mannytimes': 15261,\n",
       " 'stole': 23776,\n",
       " 'briliant': 3583,\n",
       " 'koncept': 13984,\n",
       " 'themighty': 24945,\n",
       " 'zero': 28036,\n",
       " 'kicked': 13840,\n",
       " 'ass': 2016,\n",
       " 'thingsabout': 25022,\n",
       " 'exelent': 8971,\n",
       " 'control': 5708,\n",
       " 'speedand': 23296,\n",
       " 'possibility': 18961,\n",
       " 'destroying': 6976,\n",
       " 'rival': 21099,\n",
       " 'plus': 18743,\n",
       " 'thoselovely': 25067,\n",
       " 'cars': 4212,\n",
       " 'jumps': 13651,\n",
       " 'tunnels': 25841,\n",
       " 'fast': 9367,\n",
       " 'gameplay': 10439,\n",
       " 'bother': 3359,\n",
       " 'playing': 18687,\n",
       " 'wipeout': 27548,\n",
       " 'stupid': 23991,\n",
       " 'mostly': 16312,\n",
       " 'getting': 10651,\n",
       " 'each': 8044,\n",
       " 'instead': 12927,\n",
       " 'horror': 12116,\n",
       " 'killing': 13872,\n",
       " 'anything': 1680,\n",
       " 'tell': 24763,\n",
       " 'hang': 11403,\n",
       " 'surprise': 24276,\n",
       " 'ending': 8473,\n",
       " 'windows': 27515,\n",
       " 'unstable': 26365,\n",
       " 'setup': 22135,\n",
       " 'errors': 8717,\n",
       " 'joke': 13547,\n",
       " 'personaly': 18330,\n",
       " 'stayed': 23649,\n",
       " 'beta': 2879,\n",
       " 'unreliable': 26333,\n",
       " 'installed': 12913,\n",
       " 'update': 26417,\n",
       " 'critical': 6140,\n",
       " 'updates': 26420,\n",
       " 'already': 1347,\n",
       " 'regarding': 20384,\n",
       " 'recomend': 20203,\n",
       " 'nt': 17113,\n",
       " 'kernel': 13798,\n",
       " 'becuase': 2688,\n",
       " '95': 618,\n",
       " 'stable': 23520,\n",
       " '2000': 250,\n",
       " 'xp': 27859,\n",
       " 'serious': 22096,\n",
       " 'computing': 5405,\n",
       " 'although': 1366,\n",
       " 'beach': 2622,\n",
       " 'boys': 3428,\n",
       " 'harmonize': 11475,\n",
       " 'produced': 19320,\n",
       " 'undeniably': 26080,\n",
       " 'vibrations': 26763,\n",
       " 'girls': 10722,\n",
       " 'etc': 8782,\n",
       " 'tried': 25687,\n",
       " 'vain': 26559,\n",
       " '30': 343,\n",
       " 'discover': 7320,\n",
       " 'appeal': 1735,\n",
       " 'pet': 18358,\n",
       " 'sounds': 23179,\n",
       " 'avail': 2253,\n",
       " 'those': 25066,\n",
       " 'claim': 4835,\n",
       " 'rated': 19984,\n",
       " 'collection': 5128,\n",
       " 'outdated': 17630,\n",
       " 'pop': 18872,\n",
       " 'perpetuating': 18308,\n",
       " 'hoax': 11952,\n",
       " 'rubber': 21333,\n",
       " 'soul': 23167,\n",
       " 'beatles': 2657,\n",
       " 'comparing': 5304,\n",
       " 'sopranos': 23146,\n",
       " 'godfather': 10830,\n",
       " 'honest': 12045,\n",
       " 'actually': 900,\n",
       " 'spent': 23316,\n",
       " 'heavy': 11677,\n",
       " 'rotation': 21283,\n",
       " 'prior': 19266,\n",
       " 'loved': 14897,\n",
       " 'sit': 22638,\n",
       " 'since': 22592,\n",
       " 'surprisingly': 24280,\n",
       " 'unoriginal': 26297,\n",
       " 'favor': 9403,\n",
       " 'culprit': 6272,\n",
       " 'remained': 20532,\n",
       " 'until': 26383,\n",
       " 'thankfully': 24892,\n",
       " 'arrived': 1926,\n",
       " 'eventually': 8844,\n",
       " 'three': 25087,\n",
       " 'selected': 21988,\n",
       " 'somewhat': 23094,\n",
       " 'satisfying': 21617,\n",
       " 'try': 25793,\n",
       " 'next': 16868,\n",
       " 'endeavor': 8470,\n",
       " 'courtesy': 5948,\n",
       " 'public': 19563,\n",
       " 'library': 14523,\n",
       " 'susan': 24303,\n",
       " 'placed': 18620,\n",
       " 'yankee': 27897,\n",
       " 'retail': 20838,\n",
       " 'receive': 20146,\n",
       " 'part': 17998,\n",
       " 'contacting': 5631,\n",
       " 'email': 8345,\n",
       " 'live': 14715,\n",
       " 'chat': 4534,\n",
       " 'online': 17407,\n",
       " 'answers': 1632,\n",
       " 'nor': 17011,\n",
       " 'respond': 20795,\n",
       " 'full': 10306,\n",
       " 'amazon': 1401,\n",
       " 'documentary': 7561,\n",
       " 'terrific': 24842,\n",
       " 'suffers': 24118,\n",
       " 'abysmally': 739,\n",
       " 'poor': 18869,\n",
       " 'editing': 8161,\n",
       " 'design': 6932,\n",
       " 'jump': 13647,\n",
       " 'cuts': 6354,\n",
       " 'modern': 16136,\n",
       " 'advertising': 1029,\n",
       " 'tv': 25879,\n",
       " 'technique': 24718,\n",
       " 'limiting': 14619,\n",
       " 'screen': 21825,\n",
       " 'images': 12431,\n",
       " 'seconds': 21927,\n",
       " 'quickly': 19783,\n",
       " 'entire': 8598,\n",
       " 'series': 22095,\n",
       " 'sequences': 22081,\n",
       " 'longer': 14808,\n",
       " 'presumption': 19191,\n",
       " 'viewer': 26799,\n",
       " 'attention': 2144,\n",
       " 'span': 23225,\n",
       " 'equivalent': 8681,\n",
       " 'small': 22846,\n",
       " 'depth': 6894,\n",
       " 'effort': 8205,\n",
       " 'develop': 7019,\n",
       " 'thoughtful': 25074,\n",
       " 'accounts': 804,\n",
       " 'video': 26783,\n",
       " 'footage': 9961,\n",
       " 'interviews': 13085,\n",
       " 'ends': 8482,\n",
       " 'mediocre': 15626,\n",
       " 'ready': 20064,\n",
       " 'test': 24857,\n",
       " 'teach': 24677,\n",
       " 'gives': 10734,\n",
       " 'frusterating': 10279,\n",
       " 'low': 14909,\n",
       " ...}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####REMOVE\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** You should see that the first entry is `'this': 25040`. What does the 25040 represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How many total tokens are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this in to answer the above question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####REMOVE\n",
    "len(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how often each word appeared across all reviews. To do this, we need to look at X_train_vec.\n",
    "\n",
    "**Question:** What type of object is X_train_vec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this in to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we can sum this object to get a count per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[55, 21,  2, ...,  1,  1,  1]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vec.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice to be able to view the counts result as a DataFrame. To do this, we need to convert this into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[55, 21,  2, ...,  1,  1,  1]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_train_vec.sum(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, when we make a DataFrame, we need to pass in these values as a one-dimensional object. For this, we can use the `flatten` method.\n",
    "\n",
    "Make a `word_counts` DataFrame which was a 'words' column containing each word in the vocabulary and a 'frequency' column containing the counts.\n",
    "\n",
    "**Hint:** Check the methods of the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to see how to access the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>007</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01781</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   words  frequency\n",
       "0     00         55\n",
       "1    000         21\n",
       "2    007          2\n",
       "3     01          4\n",
       "4  01781          1"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####REMOVE\n",
    "word_counts = pd.DataFrame({'words': vect.get_feature_names_out(),\n",
    "                            'frequency': np.array(X_train_vec.sum(axis = 0)).flatten()})\n",
    "\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this in to build a DataFrame of words and their counts\n",
    "word_counts = pd.DataFrame({\n",
    "    'words': #Fill this in,\n",
    "    'frequency': #Fill this in\n",
    "})\n",
    "\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which word appears most frequently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24914</th>\n",
       "      <td>the</td>\n",
       "      <td>28900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>and</td>\n",
       "      <td>15664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25263</th>\n",
       "      <td>to</td>\n",
       "      <td>14087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13280</th>\n",
       "      <td>it</td>\n",
       "      <td>13053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17294</th>\n",
       "      <td>of</td>\n",
       "      <td>11109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12588</th>\n",
       "      <td>incidentally</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12583</th>\n",
       "      <td>incestous</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12582</th>\n",
       "      <td>incest</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12581</th>\n",
       "      <td>incessant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28089</th>\n",
       "      <td>über</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28090 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              words  frequency\n",
       "24914           the      28900\n",
       "1508            and      15664\n",
       "25263            to      14087\n",
       "13280            it      13053\n",
       "17294            of      11109\n",
       "...             ...        ...\n",
       "12588  incidentally          1\n",
       "12583     incestous          1\n",
       "12582        incest          1\n",
       "12581     incessant          1\n",
       "28089          über          1\n",
       "\n",
       "[28090 rows x 2 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.sort_values('frequency', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's fit a MultinomialNB model to our word count vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB().fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.786"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1031,  219],\n",
       "       [ 316,  934]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How do the estimated probabilites $P(great | positive)$ and $P(great | negative)$ compare? How about $P(disappointed | positive)$ and $P(disappointed | negative)$? \n",
    "\n",
    "Hint: You can access the estimated (log) probabilities via the `feature_log_prob_` attribute of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00133935, 0.00444937])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### REMOVE\n",
    "np.exp(nb.feature_log_prob_)[:, vect.vocabulary_['great']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00076017, 0.00022389])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(nb.feature_log_prob_)[:, vect.vocabulary_['disappointed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the code to fit a LogisticRegression model.\n",
    "\n",
    "What accuracy score do you obtain? How does the confusion matrix look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8068"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter = 10000).fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test_vec)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1003,  247],\n",
       "       [ 236, 1014]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining the Model\n",
    "\n",
    "There are two lenses through which we can try to understand how the model is making predictions - (global) features importances and feature importances for single predictions.\n",
    "\n",
    "We'll start by looking at global explanations. Since we're using a logistic regression model, we can look at the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27722</th>\n",
       "      <td>worst</td>\n",
       "      <td>-2.631002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3333</th>\n",
       "      <td>boring</td>\n",
       "      <td>-2.437989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27155</th>\n",
       "      <td>waste</td>\n",
       "      <td>-2.110624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18869</th>\n",
       "      <td>poor</td>\n",
       "      <td>-2.107917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18870</th>\n",
       "      <td>poorly</td>\n",
       "      <td>-1.827457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>-1.768856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20875</th>\n",
       "      <td>returned</td>\n",
       "      <td>-1.664780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7124</th>\n",
       "      <td>didnt</td>\n",
       "      <td>-1.651214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7273</th>\n",
       "      <td>disappointment</td>\n",
       "      <td>-1.550021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24839</th>\n",
       "      <td>terrible</td>\n",
       "      <td>-1.531603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word  coefficient\n",
       "27722           worst    -2.631002\n",
       "3333           boring    -2.437989\n",
       "27155           waste    -2.110624\n",
       "18869            poor    -2.107917\n",
       "18870          poorly    -1.827457\n",
       "7270     disappointed    -1.768856\n",
       "20875        returned    -1.664780\n",
       "7124            didnt    -1.651214\n",
       "7273   disappointment    -1.550021\n",
       "24839        terrible    -1.531603"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients = pd.DataFrame({\n",
    "    'word': vect.get_feature_names_out(),\n",
    "    'coefficient': logreg.coef_[0]\n",
    "})\n",
    "\n",
    "coefficients.sort_values('coefficient').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8917</th>\n",
       "      <td>excellent</td>\n",
       "      <td>1.907333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9329</th>\n",
       "      <td>fantastic</td>\n",
       "      <td>1.638024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>awesome</td>\n",
       "      <td>1.615352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11855</th>\n",
       "      <td>highly</td>\n",
       "      <td>1.527134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18255</th>\n",
       "      <td>perfect</td>\n",
       "      <td>1.475142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11048</th>\n",
       "      <td>great</td>\n",
       "      <td>1.383717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14892</th>\n",
       "      <td>love</td>\n",
       "      <td>1.379172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>amazing</td>\n",
       "      <td>1.374360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5300</th>\n",
       "      <td>compare</td>\n",
       "      <td>1.369906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18701</th>\n",
       "      <td>pleased</td>\n",
       "      <td>1.349113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  coefficient\n",
       "8917   excellent     1.907333\n",
       "9329   fantastic     1.638024\n",
       "2297     awesome     1.615352\n",
       "11855     highly     1.527134\n",
       "18255    perfect     1.475142\n",
       "11048      great     1.383717\n",
       "14892       love     1.379172\n",
       "1399     amazing     1.374360\n",
       "5300     compare     1.369906\n",
       "18701    pleased     1.349113"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients.sort_values('coefficient', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** By what factor do we multiply the estimated odds of a review being positive when we see the word \"great\"? What about the word \"garbage\"? What about the word \"the\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coefficient    3.989705\n",
       "Name: great, dtype: float64"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### REMOVE\n",
    "np.exp(coefficients.set_index('word').loc['great'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coefficient    0.513755\n",
       "Name: garbage, dtype: float64"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## REMOVE\n",
    "np.exp(coefficients.set_index('word').loc['garbage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coefficient    0.985293\n",
       "Name: the, dtype: float64"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(coefficients.set_index('word').loc['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a helper function so that you can see what contributes to individual predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_features(text, vectorizer, model):\n",
    "    weight = vectorizer.transform(text).toarray().flatten() * model.coef_.flatten()\n",
    "    weights = pd.DataFrame({\n",
    "        'word': vectorizer.get_feature_names_out(),\n",
    "        'weight': weight\n",
    "    })\n",
    "    return weights[weights['weight'] != 0].sort_values('weight', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe almost everything in this book is true. I think the assumptions are correct, the assessments realistic and true. But, its impossible to accept at face value, because 100 pages of footnotes marked confidential do not a proof make. By the very nature of the subject, there is subterfuge and deception, secrecy and deceit. In the end, you have to believe a good part of this book without seeing all the footprints. And then, its scary as hell.The holy trinity of oil, prejudice and profits mean that this world won't change any time soon.\n",
      "Predicted Probability of Positive: 0.8997689621304078\n",
      "True Label: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>and</td>\n",
       "      <td>1.035374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21703</th>\n",
       "      <td>scary</td>\n",
       "      <td>0.641645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10866</th>\n",
       "      <td>good</td>\n",
       "      <td>0.629234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13300</th>\n",
       "      <td>its</td>\n",
       "      <td>0.430151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24914</th>\n",
       "      <td>the</td>\n",
       "      <td>0.420726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12526</th>\n",
       "      <td>impossible</td>\n",
       "      <td>-0.524243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2780</th>\n",
       "      <td>believe</td>\n",
       "      <td>-0.527439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8466</th>\n",
       "      <td>end</td>\n",
       "      <td>-0.588499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17857</th>\n",
       "      <td>pages</td>\n",
       "      <td>-0.903993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17043</th>\n",
       "      <td>not</td>\n",
       "      <td>-1.155915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word    weight\n",
       "1508          and  1.035374\n",
       "21703       scary  0.641645\n",
       "10866        good  0.629234\n",
       "13300         its  0.430151\n",
       "24914         the  0.420726\n",
       "...           ...       ...\n",
       "12526  impossible -0.524243\n",
       "2780      believe -0.527439\n",
       "8466          end -0.588499\n",
       "17857       pages -0.903993\n",
       "17043         not -1.155915\n",
       "\n",
       "[65 rows x 2 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "text = X_test.iloc[i]\n",
    "print(text['text'])\n",
    "\n",
    "print('Predicted Probability of Positive: {}'.format(logreg.predict_proba(X_test_vec[i].reshape(1,-1))[0,1]))\n",
    "print('True Label: {}'.format(y_test.iloc[i]))\n",
    "\n",
    "get_most_important_features(text, vect, logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some examples that are incorrecly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 26,  36,  47,  53,  54,  60,  62,  65,  87, 106])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where((y_test == 2) & (y_pred == 1))[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epesode told us bout how mac gyver never give up, even the person who he help hate him & insult him at the first he tried to help her.\n",
      "Predicted Probability of Positive: 0.02302098289037401\n",
      "True Label: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26467</th>\n",
       "      <td>us</td>\n",
       "      <td>0.490733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12167</th>\n",
       "      <td>how</td>\n",
       "      <td>0.235470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3396</th>\n",
       "      <td>bout</td>\n",
       "      <td>0.172709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27412</th>\n",
       "      <td>who</td>\n",
       "      <td>0.142080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24914</th>\n",
       "      <td>the</td>\n",
       "      <td>0.120208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11585</th>\n",
       "      <td>he</td>\n",
       "      <td>0.082552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25305</th>\n",
       "      <td>told</td>\n",
       "      <td>-0.023660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25040</th>\n",
       "      <td>this</td>\n",
       "      <td>-0.027992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>at</td>\n",
       "      <td>-0.065480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11761</th>\n",
       "      <td>her</td>\n",
       "      <td>-0.086021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>mac</td>\n",
       "      <td>-0.106287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9688</th>\n",
       "      <td>first</td>\n",
       "      <td>-0.115456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25263</th>\n",
       "      <td>to</td>\n",
       "      <td>-0.129906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18320</th>\n",
       "      <td>person</td>\n",
       "      <td>-0.188272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26411</th>\n",
       "      <td>up</td>\n",
       "      <td>-0.253229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8834</th>\n",
       "      <td>even</td>\n",
       "      <td>-0.327842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11519</th>\n",
       "      <td>hate</td>\n",
       "      <td>-0.358422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16834</th>\n",
       "      <td>never</td>\n",
       "      <td>-0.371822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12962</th>\n",
       "      <td>insult</td>\n",
       "      <td>-0.441269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11884</th>\n",
       "      <td>him</td>\n",
       "      <td>-0.454753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10728</th>\n",
       "      <td>give</td>\n",
       "      <td>-0.552839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25687</th>\n",
       "      <td>tried</td>\n",
       "      <td>-0.777960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11729</th>\n",
       "      <td>help</td>\n",
       "      <td>-0.926068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word    weight\n",
       "26467      us  0.490733\n",
       "12167     how  0.235470\n",
       "3396     bout  0.172709\n",
       "27412     who  0.142080\n",
       "24914     the  0.120208\n",
       "11585      he  0.082552\n",
       "25305    told -0.023660\n",
       "25040    this -0.027992\n",
       "2087       at -0.065480\n",
       "11761     her -0.086021\n",
       "15037     mac -0.106287\n",
       "9688    first -0.115456\n",
       "25263      to -0.129906\n",
       "18320  person -0.188272\n",
       "26411      up -0.253229\n",
       "8834     even -0.327842\n",
       "11519    hate -0.358422\n",
       "16834   never -0.371822\n",
       "12962  insult -0.441269\n",
       "11884     him -0.454753\n",
       "10728    give -0.552839\n",
       "25687   tried -0.777960\n",
       "11729    help -0.926068"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 26\n",
    "\n",
    "text = X_test.iloc[i]\n",
    "print(text['text'])\n",
    "\n",
    "print('Predicted Probability of Positive: {}'.format(logreg.predict_proba(X_test_vec[i].reshape(1,-1))[0,1]))\n",
    "print('True Label: {}'.format(y_test.iloc[i]))\n",
    "\n",
    "get_most_important_features(text, vect, logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding $n$-grams\n",
    "\n",
    "Notice that the vectorizer we are currently using only looks like words individually and does not consider the order. We can include combinations of words using n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new vectorizer that includes both unigrams and bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(# Fill this in so that the model uses both unigrams and bigrams)\n",
    "\n",
    "X_train_vec = vect.fit_transform(X_train['text'])\n",
    "X_test_vec = vect.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REMOVE\n",
    "vect = CountVectorizer(ngram_range = (1,2))\n",
    "\n",
    "X_train_vec = vect.fit_transform(X_train['text'])\n",
    "X_test_vec = vect.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How large is the vocabulary when you use unigrams and bigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257212"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1033,  217],\n",
       "       [ 210, 1040]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter = 10000).fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test_vec)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33917</th>\n",
       "      <td>boring</td>\n",
       "      <td>-1.740628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167603</th>\n",
       "      <td>poor</td>\n",
       "      <td>-1.380055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60754</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>-1.324677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243298</th>\n",
       "      <td>waste</td>\n",
       "      <td>-1.321843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253000</th>\n",
       "      <td>worst</td>\n",
       "      <td>-1.319185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146173</th>\n",
       "      <td>not</td>\n",
       "      <td>-1.115436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210688</th>\n",
       "      <td>terrible</td>\n",
       "      <td>-1.112718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24465</th>\n",
       "      <td>bad</td>\n",
       "      <td>-1.085593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147438</th>\n",
       "      <td>nothing</td>\n",
       "      <td>-1.075412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167710</th>\n",
       "      <td>poorly</td>\n",
       "      <td>-1.062365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136592</th>\n",
       "      <td>money</td>\n",
       "      <td>-1.044302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182056</th>\n",
       "      <td>returned</td>\n",
       "      <td>-1.006225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36030</th>\n",
       "      <td>broke</td>\n",
       "      <td>-0.981119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109070</th>\n",
       "      <td>instead</td>\n",
       "      <td>-0.968008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219304</th>\n",
       "      <td>the worst</td>\n",
       "      <td>-0.929707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62616</th>\n",
       "      <td>doesn</td>\n",
       "      <td>-0.928654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239054</th>\n",
       "      <td>very disappointed</td>\n",
       "      <td>-0.917246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60865</th>\n",
       "      <td>disappointment</td>\n",
       "      <td>-0.891875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147294</th>\n",
       "      <td>not worth</td>\n",
       "      <td>-0.868769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199576</th>\n",
       "      <td>sorry</td>\n",
       "      <td>-0.826945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102478</th>\n",
       "      <td>horrible</td>\n",
       "      <td>-0.805297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182004</th>\n",
       "      <td>return</td>\n",
       "      <td>-0.786436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59758</th>\n",
       "      <td>didnt</td>\n",
       "      <td>-0.773999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92587</th>\n",
       "      <td>guess</td>\n",
       "      <td>-0.763158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237485</th>\n",
       "      <td>useless</td>\n",
       "      <td>-0.759885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     word  coefficient\n",
       "33917              boring    -1.740628\n",
       "167603               poor    -1.380055\n",
       "60754        disappointed    -1.324677\n",
       "243298              waste    -1.321843\n",
       "253000              worst    -1.319185\n",
       "146173                not    -1.115436\n",
       "210688           terrible    -1.112718\n",
       "24465                 bad    -1.085593\n",
       "147438            nothing    -1.075412\n",
       "167710             poorly    -1.062365\n",
       "136592              money    -1.044302\n",
       "182056           returned    -1.006225\n",
       "36030               broke    -0.981119\n",
       "109070            instead    -0.968008\n",
       "219304          the worst    -0.929707\n",
       "62616               doesn    -0.928654\n",
       "239054  very disappointed    -0.917246\n",
       "60865      disappointment    -0.891875\n",
       "147294          not worth    -0.868769\n",
       "199576              sorry    -0.826945\n",
       "102478           horrible    -0.805297\n",
       "182004             return    -0.786436\n",
       "59758               didnt    -0.773999\n",
       "92587               guess    -0.763158\n",
       "237485            useless    -0.759885"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients = pd.DataFrame({\n",
    "    'word': vect.get_feature_names_out(),\n",
    "    'coefficient': logreg.coef_[0]\n",
    "})\n",
    "\n",
    "coefficients.sort_values('coefficient', ascending = True).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** By what factor do we multiply predicted odds of a review being positive if we see the phrase \"love this\"? What about \"very disappointed\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coefficient    0.399618\n",
       "Name: very disappointed, dtype: float64"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(coefficients.set_index(\"word\").loc['very disappointed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coefficient    1.997428\n",
       "Name: love this, dtype: float64"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(coefficients.set_index(\"word\").loc['love this'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF\n",
    "\n",
    "Instead of using raw counts, we can instead use a tfidf-vectorizer. This will scale down the weights for frequently-occuring words.\n",
    "\n",
    "The acronym \"tfidf\" stands for \"term frequency inverse document frequency\". This type of vectorizer takes into account the number of times a word occurs in a document but then scales inversely for the number of documents that word appears in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "\n",
    "X_train_vec = vect.fit_transform(X_train['text'])\n",
    "X_test_vec = vect.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1030,  220],\n",
       "       [ 224, 1026]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter = 10000).fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test_vec)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the CountVectorizer and TfidfVectorizer have some additional parameters that change the way that it treats tokens or which can remove certain tokens. \n",
    "\n",
    "Look at the `min_df` and `max_df` features. Experiment with these and see if you see any change in how the model performs when you adjust these (or any other parameters you want to experiment with).\n",
    "\n",
    "Try out some different values for these parameters. Which combination does best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_pipe = Pipeline(steps = [\n",
    "    ('vectorize', TfidfVectorizer()),\n",
    "    ('logreg', LogisticRegression(max_iter = 10000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'vectorize__min_df': [1, 5, 10, 25],\n",
    "    'vectorize__max_df': [1.0, 0.75, 0.5],\n",
    "    'vectorize__ngram_range': [(1,1), (1,2), (1,3)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('vectorize', TfidfVectorizer()),\n",
       "                                       ('logreg',\n",
       "                                        LogisticRegression(max_iter=10000))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'vectorize__max_df': [1.0, 0.75, 0.5],\n",
       "                         'vectorize__min_df': [1, 5, 10, 25],\n",
       "                         'vectorize__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(estimator = classification_pipe,\n",
    "                    param_grid = param_grid, verbose = 2, n_jobs = -1)\n",
    "\n",
    "grid.fit(X_train['text'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorize__max_df': 0.75,\n",
       " 'vectorize__min_df': 5,\n",
       " 'vectorize__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1047,  203],\n",
       "       [ 215, 1035]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=1, vectorize__ngram_range=(1, 1); total time=   1.0s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=1, vectorize__ngram_range=(1, 2); total time=   4.5s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=1, vectorize__ngram_range=(1, 3); total time=  11.1s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=5, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=5, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=5, vectorize__ngram_range=(1, 2); total time=   1.7s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=5, vectorize__ngram_range=(1, 2); total time=   1.7s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=5, vectorize__ngram_range=(1, 3); total time=   3.8s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=10, vectorize__ngram_range=(1, 1); total time=   0.9s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=10, vectorize__ngram_range=(1, 1); total time=   0.9s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=10, vectorize__ngram_range=(1, 2); total time=   2.0s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=10, vectorize__ngram_range=(1, 3); total time=   3.1s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=25, vectorize__ngram_range=(1, 1); total time=   0.7s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=25, vectorize__ngram_range=(1, 1); total time=   0.9s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=25, vectorize__ngram_range=(1, 2); total time=   1.8s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=25, vectorize__ngram_range=(1, 3); total time=   2.7s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=25, vectorize__ngram_range=(1, 3); total time=   2.7s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=1, vectorize__ngram_range=(1, 2); total time=   4.4s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=1, vectorize__ngram_range=(1, 3); total time=   9.4s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=5, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=5, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=5, vectorize__ngram_range=(1, 2); total time=   1.6s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=5, vectorize__ngram_range=(1, 2); total time=   1.8s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=5, vectorize__ngram_range=(1, 3); total time=   3.1s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=5, vectorize__ngram_range=(1, 3); total time=   3.2s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=10, vectorize__ngram_range=(1, 2); total time=   1.7s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=10, vectorize__ngram_range=(1, 3); total time=   2.7s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=25, vectorize__ngram_range=(1, 1); total time=   0.7s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=25, vectorize__ngram_range=(1, 1); total time=   0.7s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=25, vectorize__ngram_range=(1, 2); total time=   1.7s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=25, vectorize__ngram_range=(1, 3); total time=   3.1s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=1, vectorize__ngram_range=(1, 1); total time=   1.0s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=1, vectorize__ngram_range=(1, 1); total time=   1.2s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=1, vectorize__ngram_range=(1, 2); total time=   4.6s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=1, vectorize__ngram_range=(1, 2); total time=   3.8s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=1, vectorize__ngram_range=(1, 3); total time=   8.5s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=5, vectorize__ngram_range=(1, 2); total time=   1.7s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=5, vectorize__ngram_range=(1, 3); total time=   2.8s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=5, vectorize__ngram_range=(1, 3); total time=   2.8s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=10, vectorize__ngram_range=(1, 2); total time=   2.4s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=10, vectorize__ngram_range=(1, 3); total time=   3.4s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=25, vectorize__ngram_range=(1, 1); total time=   0.7s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=25, vectorize__ngram_range=(1, 1); total time=   0.9s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=25, vectorize__ngram_range=(1, 2); total time=   1.6s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=25, vectorize__ngram_range=(1, 3); total time=   2.6s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=1, vectorize__ngram_range=(1, 1); total time=   1.1s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=1, vectorize__ngram_range=(1, 2); total time=   4.4s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=1, vectorize__ngram_range=(1, 3); total time=  11.1s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=5, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=5, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=5, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=5, vectorize__ngram_range=(1, 2); total time=   1.8s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=5, vectorize__ngram_range=(1, 3); total time=   3.3s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=5, vectorize__ngram_range=(1, 3); total time=   3.2s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=10, vectorize__ngram_range=(1, 2); total time=   1.7s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=10, vectorize__ngram_range=(1, 2); total time=   1.7s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=10, vectorize__ngram_range=(1, 3); total time=   2.8s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=25, vectorize__ngram_range=(1, 1); total time=   0.7s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=25, vectorize__ngram_range=(1, 2); total time=   1.7s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=25, vectorize__ngram_range=(1, 3); total time=   2.7s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=1, vectorize__ngram_range=(1, 1); total time=   1.1s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=1, vectorize__ngram_range=(1, 1); total time=   1.0s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=1, vectorize__ngram_range=(1, 2); total time=   4.3s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=1, vectorize__ngram_range=(1, 2); total time=   3.7s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=1, vectorize__ngram_range=(1, 3); total time=   9.3s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=5, vectorize__ngram_range=(1, 2); total time=   1.8s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=5, vectorize__ngram_range=(1, 3); total time=   3.2s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=5, vectorize__ngram_range=(1, 3); total time=   3.2s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=10, vectorize__ngram_range=(1, 2); total time=   1.6s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=10, vectorize__ngram_range=(1, 3); total time=   2.8s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=25, vectorize__ngram_range=(1, 1); total time=   0.7s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=25, vectorize__ngram_range=(1, 1); total time=   0.7s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=25, vectorize__ngram_range=(1, 2); total time=   1.7s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=25, vectorize__ngram_range=(1, 2); total time=   1.8s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=25, vectorize__ngram_range=(1, 3); total time=   2.8s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=1, vectorize__ngram_range=(1, 1); total time=   1.2s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=1, vectorize__ngram_range=(1, 2); total time=   4.4s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=1, vectorize__ngram_range=(1, 3); total time=   8.0s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=1, vectorize__ngram_range=(1, 3); total time=   7.9s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=5, vectorize__ngram_range=(1, 3); total time=   2.7s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=10, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=10, vectorize__ngram_range=(1, 2); total time=   2.3s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=10, vectorize__ngram_range=(1, 3); total time=   2.9s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=10, vectorize__ngram_range=(1, 3); total time=   2.9s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=25, vectorize__ngram_range=(1, 2); total time=   1.5s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=25, vectorize__ngram_range=(1, 3); total time=   2.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=1, vectorize__ngram_range=(1, 1); total time=   1.0s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=1, vectorize__ngram_range=(1, 2); total time=   4.0s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=1, vectorize__ngram_range=(1, 2); total time=   3.9s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=1, vectorize__ngram_range=(1, 3); total time=   9.9s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=5, vectorize__ngram_range=(1, 2); total time=   1.5s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=5, vectorize__ngram_range=(1, 2); total time=   1.8s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=5, vectorize__ngram_range=(1, 3); total time=   3.4s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=10, vectorize__ngram_range=(1, 1); total time=   0.9s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=10, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=10, vectorize__ngram_range=(1, 2); total time=   2.0s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=10, vectorize__ngram_range=(1, 3); total time=   3.0s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=25, vectorize__ngram_range=(1, 1); total time=   0.7s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=25, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=25, vectorize__ngram_range=(1, 2); total time=   1.7s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=25, vectorize__ngram_range=(1, 2); total time=   1.6s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=25, vectorize__ngram_range=(1, 3); total time=   2.7s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=1, vectorize__ngram_range=(1, 1); total time=   1.0s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=1, vectorize__ngram_range=(1, 2); total time=   4.1s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=1, vectorize__ngram_range=(1, 3); total time=   9.2s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=1, vectorize__ngram_range=(1, 3); total time=   9.0s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=10, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=10, vectorize__ngram_range=(1, 1); total time=   0.7s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=10, vectorize__ngram_range=(1, 1); total time=   0.7s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=10, vectorize__ngram_range=(1, 2); total time=   1.6s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=10, vectorize__ngram_range=(1, 3); total time=   2.8s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=10, vectorize__ngram_range=(1, 3); total time=   2.7s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=25, vectorize__ngram_range=(1, 2); total time=   1.6s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=25, vectorize__ngram_range=(1, 3); total time=   3.1s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=1, vectorize__ngram_range=(1, 1); total time=   1.1s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=1, vectorize__ngram_range=(1, 1); total time=   1.2s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=1, vectorize__ngram_range=(1, 2); total time=   4.9s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=1, vectorize__ngram_range=(1, 3); total time=   8.8s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=5, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=5, vectorize__ngram_range=(1, 1); total time=   0.7s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=5, vectorize__ngram_range=(1, 2); total time=   1.8s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=5, vectorize__ngram_range=(1, 2); total time=   1.8s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=5, vectorize__ngram_range=(1, 3); total time=   2.8s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=10, vectorize__ngram_range=(1, 1); total time=   0.7s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=10, vectorize__ngram_range=(1, 1); total time=   0.7s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=10, vectorize__ngram_range=(1, 2); total time=   2.1s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=10, vectorize__ngram_range=(1, 2); total time=   2.1s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=10, vectorize__ngram_range=(1, 3); total time=   3.0s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=25, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=25, vectorize__ngram_range=(1, 2); total time=   1.8s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=25, vectorize__ngram_range=(1, 2); total time=   1.5s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=25, vectorize__ngram_range=(1, 3); total time=   2.1s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=1, vectorize__ngram_range=(1, 1); total time=   0.9s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=1, vectorize__ngram_range=(1, 1); total time=   0.9s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=1, vectorize__ngram_range=(1, 2); total time=   4.4s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=1, vectorize__ngram_range=(1, 3); total time=   9.5s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=1, vectorize__ngram_range=(1, 3); total time=   8.1s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=5, vectorize__ngram_range=(1, 3); total time=   3.0s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=10, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=10, vectorize__ngram_range=(1, 2); total time=   1.7s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=10, vectorize__ngram_range=(1, 3); total time=   2.8s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=10, vectorize__ngram_range=(1, 3); total time=   3.0s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=25, vectorize__ngram_range=(1, 2); total time=   1.6s\n",
      "[CV] END vectorize__max_df=1.0, vectorize__min_df=25, vectorize__ngram_range=(1, 3); total time=   2.6s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=1, vectorize__ngram_range=(1, 1); total time=   0.9s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=1, vectorize__ngram_range=(1, 1); total time=   0.9s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=1, vectorize__ngram_range=(1, 2); total time=   4.4s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=1, vectorize__ngram_range=(1, 3); total time=   9.0s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=5, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=5, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=5, vectorize__ngram_range=(1, 1); total time=   0.9s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=5, vectorize__ngram_range=(1, 2); total time=   1.9s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=5, vectorize__ngram_range=(1, 2); total time=   1.9s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=5, vectorize__ngram_range=(1, 3); total time=   3.0s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=10, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=10, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=10, vectorize__ngram_range=(1, 2); total time=   1.7s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=10, vectorize__ngram_range=(1, 2); total time=   1.7s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=10, vectorize__ngram_range=(1, 3); total time=   2.9s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=25, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=25, vectorize__ngram_range=(1, 2); total time=   1.7s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=25, vectorize__ngram_range=(1, 3); total time=   2.8s\n",
      "[CV] END vectorize__max_df=0.75, vectorize__min_df=25, vectorize__ngram_range=(1, 3); total time=   2.9s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=1, vectorize__ngram_range=(1, 2); total time=   4.8s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=1, vectorize__ngram_range=(1, 3); total time=   8.7s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=5, vectorize__ngram_range=(1, 1); total time=   0.7s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=5, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=5, vectorize__ngram_range=(1, 1); total time=   0.7s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=5, vectorize__ngram_range=(1, 2); total time=   1.8s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=5, vectorize__ngram_range=(1, 2); total time=   1.7s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=5, vectorize__ngram_range=(1, 3); total time=   2.7s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=10, vectorize__ngram_range=(1, 1); total time=   0.7s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=10, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=10, vectorize__ngram_range=(1, 2); total time=   2.5s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=10, vectorize__ngram_range=(1, 3); total time=   3.5s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=25, vectorize__ngram_range=(1, 1); total time=   0.7s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=25, vectorize__ngram_range=(1, 1); total time=   0.8s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=25, vectorize__ngram_range=(1, 2); total time=   1.8s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=25, vectorize__ngram_range=(1, 3); total time=   2.5s\n",
      "[CV] END vectorize__max_df=0.5, vectorize__min_df=25, vectorize__ngram_range=(1, 3); total time=   1.5s\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(X_test['text'])\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Exploring Other Potential Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Does the length of a review seem to be related to its sentiment? That is, do longer (or shorter) reviews tend to have a more positive sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a \"length\" column that contains the number of characters in the review text. Also, create a \"num_words\" column that counts the number of words and a \"num_cap\" that counts the number of capital letters in the text of a review. \n",
    "\n",
    "How well do these features work for predicting whether a review is positive or negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['text'].str.len().groupby(y_train).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['length'] = X_train['text'].str.len()\n",
    "X_test['length'] = X_test['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['num_words'] = X_train['text'].str.split().apply(lambda x: len(x))\n",
    "X_test['num_words'] = X_test['text'].str.split().apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the number of capital letters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['num_cap'] = X_train['text'].str.count(r'[A-Z]')\n",
    "X_test['num_cap'] = X_test['text'].str.count(r'[A-Z]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.groupby(y_train)['num_cap'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even use pretrained models to help us. For example, nltk includes tools for determining sentiment, like the VADER tool. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. \n",
    " \n",
    "This outputs a dictionary of scores:\n",
    "* neg: proportion of words that are negative\n",
    "* neu: proportion of words that are neutral\n",
    "* pos: proportion of words that are positive\n",
    "* compound: computed by summing the valence scores of each word, normalized to be between -1 and +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was probally the 2nd best movie ive seen. The acting was great and Josh Jackson was hotter than ever. This is a must see movie.\n",
      "{'neg': 0.0, 'neu': 0.735, 'pos': 0.265, 'compound': 0.8519}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(X_train.iloc[i,0])\n",
    "print(sent_analyzer.polarity_scores(X_train.iloc[i,0]))\n",
    "print(y_train.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new columns to hold the \"neg\", \"neu\", \"pos\" and \"compound\" scores for each review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_train = X_train['text'].apply(lambda x: sent_analyzer.polarity_scores(x)).apply(pd.Series)\n",
    "vader_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train, vader_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_test = X_test['text'].apply(lambda x: sent_analyzer.polarity_scores(x)).apply(pd.Series)\n",
    "X_test = pd.concat([X_test, vader_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter = 10000).fit(X_train.drop(columns = 'text'), y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test.drop(columns = 'text'))\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = pd.DataFrame({\n",
    "    'feature': clf.feature_names_in_,\n",
    "    'coefficient': clf.coef_[0]\n",
    "})\n",
    "\n",
    "coefficients.sort_values('coefficient', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, concatenate your word counts and the new features together. How well do all of these together do at predicting positive or negative sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = pd.concat([X_train.drop(columns = 'text'),\n",
    "           pd.DataFrame(X_train_vec.toarray(), columns = vect.vocabulary_).set_index(X_train.index)],\n",
    "          axis = 1)\n",
    "\n",
    "X_test_full = pd.concat([X_test.drop(columns = 'text'),\n",
    "           pd.DataFrame(X_test_vec.toarray(), columns = vect.vocabulary_).set_index(X_test.index)],\n",
    "          axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter = 10000).fit(X_train_full, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_full)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = pd.DataFrame({\n",
    "    'word': clf.feature_names_in_,\n",
    "    'coefficient': clf.coef_[0]\n",
    "})\n",
    "\n",
    "coefficients.sort_values('coefficient', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Bonus - Using the Title and Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of ways that you could incorporate both the title and text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 1:** Concatenate together the title and the text into a single field.\n",
    "\n",
    "**Option 2:** Use a separate vectorizer for the title and text and concatenate the results.\n",
    "\n",
    "Try both options. Which gives a better result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reviews['title'] + ' ' + reviews['text']\n",
    "X = pd.DataFrame(X, columns = ['text'])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "\n",
    "X_train_vec = vect.fit_transform(X_train['text'])\n",
    "X_test_vec = vect.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter = 10000).fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2:** Vectorizer separately and concatenate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reviews[['title', 'text']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_vect = TfidfVectorizer()\n",
    "\n",
    "X_train_vec_title = title_vect.fit_transform(X_train['title'])\n",
    "X_test_vec_title = title_vect.transform(X_test['title'])\n",
    "\n",
    "\n",
    "text_vect = TfidfVectorizer()\n",
    "\n",
    "X_train_vec_text = text_vect.fit_transform(X_train['text'])\n",
    "X_test_vec_text = text_vect.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = hstack([X_train_vec_title, X_train_vec_text])\n",
    "X_test_vec = hstack([X_test_vec_title, X_test_vec_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter = 10000).fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['length'] = X_train['text'].str.len()\n",
    "X_test['length'] = X_test['text'].str.len()\n",
    "\n",
    "X_train['num_words'] = X_train['text'].str.split().apply(lambda x: len(x))\n",
    "X_test['num_words'] = X_test['text'].str.split().apply(lambda x: len(x))\n",
    "\n",
    "X_train['num_cap'] = X_train['text'].str.count(r'[A-Z]')\n",
    "X_test['num_cap'] = X_test['text'].str.count(r'[A-Z]')\n",
    "\n",
    "\n",
    "vader_train = X_train['text'].apply(lambda x: sent_analyzer.polarity_scores(x)).apply(pd.Series).rename(columns = lambda x: 'text_' + x)\n",
    "X_train = pd.concat([X_train, vader_train], axis = 1)\n",
    "vader_test = X_test['text'].apply(lambda x: sent_analyzer.polarity_scores(x)).apply(pd.Series).rename(columns = lambda x: 'text_' + x)\n",
    "X_test = pd.concat([X_test, vader_test], axis = 1)\n",
    "\n",
    "vader_train = X_train['title'].apply(lambda x: sent_analyzer.polarity_scores(x)).apply(pd.Series).rename(columns = lambda x: 'title_' + x)\n",
    "X_train = pd.concat([X_train, vader_train], axis = 1)\n",
    "vader_test = X_test['title'].apply(lambda x: sent_analyzer.polarity_scores(x)).apply(pd.Series).rename(columns = lambda x: 'title_' + x)\n",
    "X_test = pd.concat([X_test, vader_test], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = pd.concat([X_train.drop(columns = ['text', 'title']),\n",
    "           pd.DataFrame(X_train_vec.toarray(), \n",
    "                        columns = list(title_vect.vocabulary_.keys()) + list(text_vect.vocabulary_.keys())).set_index(X_train.index)],\n",
    "          axis = 1)\n",
    "\n",
    "X_test_full = pd.concat([X_test.drop(columns = ['text', 'title']),\n",
    "           pd.DataFrame(X_test_vec.toarray(), \n",
    "                        columns = list(title_vect.vocabulary_.keys()) + list(text_vect.vocabulary_.keys())).set_index(X_test.index)],\n",
    "          axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter = 10000).fit(X_train_full, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_full)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
