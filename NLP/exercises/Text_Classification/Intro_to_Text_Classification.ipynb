{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be working with a set of reviews from Amazon.com. This is a subset of a larger dataset obtained from https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('data/amazon_reviews.csv')\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each review is assigned a sentiment score, where 1 indicates negative and 2 indicates positive.\n",
    "\n",
    "This subset is equally balanced between positive and negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at some of the negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "for statement in reviews.loc[reviews['sentiment'] == 1, 'text'].sample(3, random_state=seed):\n",
    "    print(statement)\n",
    "    print('-----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then some of the positive ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "for statement in reviews.loc[reviews['sentiment'] == 2, 'text'].sample(3, random_state=seed):\n",
    "    print(statement)\n",
    "    print('-----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Naive Bayes Using the Text Field\n",
    "\n",
    "First, we'll split out data into a train and test set, stratifying on the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reviews[['text']]\n",
    "y = reviews['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way to convert the text into predictors. We will start by using a **bag of words** model - one which looks at what words are present but disregarding word order.\n",
    "\n",
    "Let's start with a [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "Fill in the code to import the CountVectorizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this in to import the CountVectorizer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a CountVectorizer (with all of the default arguments) named `vect` and fit it to the \"text\" column of the training data transform both the train and test texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the code to fit and transform a CountVectorizer (using all defaults) on the text column of X_train and X_test\n",
    "\n",
    "vect = # Fill this in\n",
    "\n",
    "X_train_vec = # Fill this in\n",
    "X_test_vec = # Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CountVectorizer class will take in the text, and **tokenize** it, splitting it into a list of tokens. The built-in tokenizer does some minimal cleaning of the text, and by default the CountVectorizer will convert all text to lowercase.\n",
    "\n",
    "If we want to take a look at all of the tokens that the CountVectorizer has seen, we can look at its vocabulary. Check the `vocabulary_` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** You should see that the first entry is `'this': 25040`. What does the 25040 represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How many total tokens are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this in to answer the above question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how often each word appeared across all reviews. To do this, we need to look at X_train_vec.\n",
    "\n",
    "**Question:** What type of object is X_train_vec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this in to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we can sum this object to get a count per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice to be able to view the counts result as a DataFrame. To do this, we need to convert this into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(X_train_vec.sum(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, when we make a DataFrame, we need to pass in these values as a one-dimensional object. For this, we can use the `flatten` method.\n",
    "\n",
    "Make a `word_counts` DataFrame which was a 'words' column containing each word in the vocabulary and a 'frequency' column containing the counts.\n",
    "\n",
    "**Hint:** Check the methods of the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to see how to access the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this in to build a DataFrame of words and their counts\n",
    "word_counts = pd.DataFrame({\n",
    "    'words': #Fill this in,\n",
    "    'frequency': #Fill this in\n",
    "})\n",
    "\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which word appears most frequently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's fit a MultinomialNB model to our word count vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB().fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How do the estimated probabilites $P(great | positive)$ and $P(great | negative)$ compare? How about $P(disappointed | positive)$ and $P(disappointed | negative)$? \n",
    "\n",
    "Hint: You can access the estimated (log) probabilities via the `feature_log_prob_` attribute of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the code to fit a LogisticRegression model.\n",
    "\n",
    "What accuracy score do you obtain? How does the confusion matrix look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter = 10000).fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test_vec)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining the Model\n",
    "\n",
    "There are two lenses through which we can try to understand how the model is making predictions - (global) features importances and feature importances for single predictions.\n",
    "\n",
    "We'll start by looking at global explanations. Since we're using a logistic regression model, we can look at the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = pd.DataFrame({\n",
    "    'word': vect.get_feature_names_out(),\n",
    "    'coefficient': logreg.coef_[0]\n",
    "})\n",
    "\n",
    "coefficients.sort_values('coefficient').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients.sort_values('coefficient', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** By what factor do we multiply the estimated odds of a review being positive when we see the word \"great\"? What about the word \"garbage\"? What about the word \"the\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a helper function so that you can see what contributes to individual predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_features(text, vectorizer, model):\n",
    "    weight = vectorizer.transform(text).toarray().flatten() * model.coef_.flatten()\n",
    "    weights = pd.DataFrame({\n",
    "        'word': vectorizer.get_feature_names_out(),\n",
    "        'weight': weight\n",
    "    })\n",
    "    return weights[weights['weight'] != 0].sort_values('weight', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "text = X_test.iloc[i]\n",
    "print(text['text'])\n",
    "\n",
    "print('Predicted Probability of Positive: {}'.format(logreg.predict_proba(X_test_vec[i].reshape(1,-1))[0,1]))\n",
    "print('True Label: {}'.format(y_test.iloc[i]))\n",
    "\n",
    "get_most_important_features(text, vect, logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some examples that are incorrecly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where((y_test == 2) & (y_pred == 1))[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 26\n",
    "\n",
    "text = X_test.iloc[i]\n",
    "print(text['text'])\n",
    "\n",
    "print('Predicted Probability of Positive: {}'.format(logreg.predict_proba(X_test_vec[i].reshape(1,-1))[0,1]))\n",
    "print('True Label: {}'.format(y_test.iloc[i]))\n",
    "\n",
    "get_most_important_features(text, vect, logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding $n$-grams\n",
    "\n",
    "Notice that the vectorizer we are currently using only looks like words individually and does not consider the order. We can include combinations of words using n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new vectorizer that includes both unigrams and bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(# Fill this in so that the model uses both unigrams and bigrams)\n",
    "\n",
    "X_train_vec = vect.fit_transform(X_train['text'])\n",
    "X_test_vec = vect.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How large is the vocabulary when you use unigrams and bigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter = 10000).fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test_vec)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = pd.DataFrame({\n",
    "    'word': vect.get_feature_names_out(),\n",
    "    'coefficient': logreg.coef_[0]\n",
    "})\n",
    "\n",
    "coefficients.sort_values('coefficient', ascending = True).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** By what factor do we multiply predicted odds of a review being positive if we see the phrase \"love this\"? What about \"very disappointed\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF\n",
    "\n",
    "Instead of using raw counts, we can instead use a tfidf-vectorizer. This will scale down the weights for frequently-occuring words.\n",
    "\n",
    "The acronym \"tfidf\" stands for \"term frequency inverse document frequency\". This type of vectorizer takes into account the number of times a word occurs in a document but then scales inversely for the number of documents that word appears in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "\n",
    "X_train_vec = vect.fit_transform(X_train['text'])\n",
    "X_test_vec = vect.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter = 10000).fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test_vec)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the CountVectorizer and TfidfVectorizer have some additional parameters that change the way that it treats tokens or which can remove certain tokens. \n",
    "\n",
    "Look at the `min_df` and `max_df` features. Experiment with these and see if you see any change in how the model performs when you adjust these (or any other parameters you want to experiment with).\n",
    "\n",
    "Try out some different values for these parameters. Which combination does best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Exploring Other Potential Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Does the length of a review seem to be related to its sentiment? That is, do longer (or shorter) reviews tend to have a more positive sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code here"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create a \"length\" column that contains the number of characters in the review text. Also, create a \"num_words\" column that counts the number of words and a \"num_cap\" that counts the number of capital letters in the text of a review. \n",
    "\n",
    "How well do these features work for predicting whether a review is positive or negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even use pretrained models to help us. For example, nltk includes tools for determining sentiment, like the VADER tool. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. \n",
    " \n",
    "This outputs a dictionary of scores:\n",
    "* neg: proportion of words that are negative\n",
    "* neu: proportion of words that are neutral\n",
    "* pos: proportion of words that are positive\n",
    "* compound: computed by summing the valence scores of each word, normalized to be between -1 and +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(X_train.iloc[i,0])\n",
    "print(sent_analyzer.polarity_scores(X_train.iloc[i,0]))\n",
    "print(y_train.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new columns to hold the \"neg\", \"neu\", \"pos\" and \"compound\" scores for each review. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, concatenate your word counts and the new features together. How well do all of these together do at predicting positive or negative sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Bonus - Using the Title and Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of ways that you could incorporate both the title and text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 1:** Concatenate together the title and the text into a single field.\n",
    "\n",
    "**Option 2:** Use a separate vectorizer for the title and text and concatenate the results.\n",
    "\n",
    "Try both options. Which gives a better result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
