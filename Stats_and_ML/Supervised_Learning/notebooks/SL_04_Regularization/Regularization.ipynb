{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "greater-dance",
   "metadata": {},
   "source": [
    "# Regularized Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-excitement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-effort",
   "metadata": {},
   "source": [
    "Let's look at a couple of models built on the King County housing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "kc = pd.read_csv('data/kc_house_data.csv')\n",
    "\n",
    "X = kc[['date', 'bedrooms', 'bathrooms', 'sqft_living',\n",
    "       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n",
    "       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n",
    "       'lat', 'long', 'sqft_living15', 'sqft_lot15']].copy()\n",
    "y = kc['price']\n",
    "\n",
    "X['date'] = pd.to_datetime(X['date'])\n",
    "X['sales_year'] = X['date'].dt.year\n",
    "\n",
    "X['age_at_sale'] = X['sales_year'] - X['yr_built']\n",
    "X['years_since_renovation'] = X['sales_year'] - np.max(X[['yr_built', 'yr_renovated']], axis = 1)\n",
    "\n",
    "\n",
    "X = X.drop(columns = ['date', 'sales_year', 'yr_built', 'yr_renovated'])\n",
    "\n",
    "X = pd.get_dummies(X, columns = ['zipcode'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-sector",
   "metadata": {},
   "source": [
    "Let's start by just making a basic linear regression model with minimal preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56671652",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R2 Score: {r2_score(y_test, linreg.predict(X_test))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-metallic",
   "metadata": {},
   "source": [
    "Let's look at the prediction we get on a particular house, say the first one in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-testimony",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_num = 0\n",
    "sample = X_test.iloc[house_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Prediction: {\"${:,.2f}\".format(linreg.predict(sample.values.reshape(1,-1))[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-guidance",
   "metadata": {},
   "source": [
    "How can we understand how the model came up with this prediction? It is a linear regression model, which means that we have coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = pd.DataFrame({'feature': X.columns,\n",
    "                             'coefficient': linreg.coef_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-upgrade",
   "metadata": {},
   "source": [
    "To get the prediction, you just multiply each feature by the corresponding coefficient and sum up the result. Let's import a helper function to better see this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from explainer import tell_me_why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_num = 0\n",
    "tell_me_why(linreg, X_test.iloc[house_num], X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-peripheral",
   "metadata": {},
   "source": [
    "We can see that this model has some ridiculous coefficients. This has to do with the fact that we have strong multicollinearity in our dataset, so the coefficients can grow quite large.\n",
    "\n",
    "We can remedy this (and often get better generalizability) by using a regularized model.\n",
    "\n",
    "Remember that ridge and lasso both use the magnitude of the coefficients in their penalty terms. For this reason, we should use a StandardScaler to scale our variables prior to fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682a1206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Pipeline(\n",
    "    steps = [\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('linreg', RidgeCV())\n",
    "    ]\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "lasso = Pipeline(\n",
    "    steps = [\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('linreg', LassoCV())\n",
    "    ]\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855ad680",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc11df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Ridge R2 Score: {r2_score(y_test, ridge.predict(X_test))}')\n",
    "print(f'Lasso R2 Score: {r2_score(y_test, lasso.predict(X_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb072fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from explainer import tell_me_why_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_num = 0\n",
    "tell_me_why_pipe(ridge, X_test.iloc[house_num], X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_num = 0\n",
    "tell_me_why_pipe(lasso, X_test.iloc[house_num], X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6837d",
   "metadata": {},
   "source": [
    "We can also manually set the penalty coefficient if we want to build a simpler model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d4873",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_manual = Pipeline(steps = \n",
    "                       [\n",
    "                           ('scaler', StandardScaler()),\n",
    "                           ('linreg', Lasso(alpha = 10000))\n",
    "                       ]\n",
    "                       ).fit(X_train, y_train)\n",
    "\n",
    "print(f'Percent of nonzero Coefficients: {(lasso_manual[-1].coef_ != 0).mean()}')\n",
    "\n",
    "print(f'Lasso R2 Score: {r2_score(y_test, lasso_manual.predict(X_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5902dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_num = 0\n",
    "tell_me_why_pipe(lasso_manual, X_test.iloc[house_num], X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab06cfe",
   "metadata": {},
   "source": [
    "Finally, we can try a \"relaxed lasso\" (https://glmnet.stanford.edu/articles/relax.html) where we refit a non-penalized model on just the variables that had nonzero coefficients. (There is actually a little more to the relaxed lasso, but we'll just do this simple version of it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = pd.DataFrame({'feature': X.columns,\n",
    "                             'coefficient': lasso_manual[-1].coef_})\n",
    "\n",
    "nonzeros = coefficients.loc[coefficients['coefficient'] != 0, 'feature']\n",
    "\n",
    "X_relaxed = X[nonzeros]\n",
    "\n",
    "X_train_relaxed, X_test_relaxed, y_train, y_test = train_test_split(X_relaxed, y, random_state = 321)\n",
    "\n",
    "lasso_relaxed = LinearRegression().fit(X_train_relaxed, y_train)\n",
    "\n",
    "print(f'Relaxed Lasso R2 Score: {r2_score(y_test, lasso_relaxed.predict(X_test_relaxed))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
